# -*- coding: utf-8 -*-
"""
üöÄ AI TASK EXTRACTION SYSTEM - API COMPL√àTE
==========================================

Syst√®me intelligent d'extraction de t√¢ches depuis les emails
Toutes les am√©liorations int√©gr√©es et fonctionnelles

Version: 2.0.0 - Production Ready
Usage: python -m uvicorn main:app --reload --port 8000
"""

from fastapi import FastAPI, HTTPException, Query
from pydantic import BaseModel, validator
from typing import List, Optional, Dict, Any
from datetime import datetime
import json
import os
import uuid
from contextlib import asynccontextmanager
import sys
from fastapi_mail import FastMail, MessageSchema, ConnectionConfig

# Configuration Email
email_conf = ConnectionConfig(
    MAIL_USERNAME="your_email@example.com",  # Remplacer par votre email
    MAIL_PASSWORD="your_password",  # Remplacer par votre mot de passe
    MAIL_FROM="your_email@example.com",  # Remplacer par votre email
    MAIL_PORT=587,
    MAIL_SERVER="smtp.gmail.com",  # Ou votre serveur SMTP
    MAIL_STARTTLS=True,
    MAIL_SSL_TLS=False,
    USE_CREDENTIALS=True,
    VALIDATE_CERTS=True
)

# Instance FastMail
fastmail = FastMail(email_conf)

# Ajouter le dossier src au path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "src"))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "src", "core"))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "src", "utils"))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "src", "services"))

# Chemins d√©j√† ajout√©s ci-dessus

# Imports des modules existants  
try:
    import agent_task
    from agent_task import (
        extract_tasks_from_email,
        resume_email,
        identifier_departement,
        deduire_priorite,
        suggere_taches_implicites
    )
    import pipeline
    from pipeline import traiter_emails
    import background_service
    from background_service import get_background_service
    import cache_emails
    from cache_emails import (
        calculer_hash_email,
        est_email_deja_traite,
        marquer_email_traite,
        obtenir_statistiques_cache,
        nettoyer_cache_ancien,
        obtenir_info_cache
    )
    import rate_limiter
    from rate_limiter import RateLimiter
    import email_queue
    from email_queue import EmailQueue
    # üéØ NOUVEAU: Import du syst√®me de r√©unions
    import meeting_processor
    from meeting_processor import get_meeting_processor, traiter_reunions
    from pipeline import traiter_reunions as pipeline_traiter_reunions
    
    MODULES_DISPONIBLES = True
    print("‚úÖ Tous les modules import√©s avec succ√®s")
    
except ImportError as e:
    print(f"‚ö†Ô∏è  Import warning: {e}")
    print("üîÑ Mode de compatibilit√© activ√© - fonctionnalit√©s de base disponibles")
    MODULES_DISPONIBLES = False

# Fonctions de fallback si les modules ne sont pas disponibles
if not MODULES_DISPONIBLES:
    def extract_tasks_from_email(texte):
        return json.dumps([{"description": "Fonction d'extraction IA non disponible", "responsable": "syst√®me", "priorite": "moyenne"}])
    
    def resume_email(texte):
        return "R√©sum√© non disponible"
    
    def identifier_departement(texte):
        return "Non identifi√©"
    
    def deduire_priorite(description):
        return "moyenne"
    
    def suggere_taches_implicites(texte):
        return json.dumps([{"description": "Suggestion de t√¢ches non disponible", "responsable": "syst√®me", "priorite": "moyenne"}])
    
    def traiter_emails(**kwargs):
        return {"message": "Pipeline non disponible", "status": "limited_mode"}
    
    def calculer_hash_email(texte, objet=""):
        import hashlib
        return hashlib.md5((texte + objet).encode()).hexdigest()[:16]
    
    def est_email_deja_traite(hash_email):
        return False
    
    def marquer_email_traite(hash_email, info):
        pass
    
    def obtenir_statistiques_cache():
        return {"total_emails": 0, "cache_hits": 0, "cache_misses": 0}
    
    def nettoyer_cache_ancien(retention_days):
        pass
    
    def obtenir_info_cache(hash_email):
        return None
    
    def get_background_service():
        class MockService:
            def start_service(self): return False
            def stop_service(self): return False
            def get_service_status(self): return {"service_running": False, "mode": "mock"}
        return MockService()
    
    class RateLimiter:
        def get_current_stats(self):
            return {"requests_made": 0, "requests_remaining": 100}
    
    class EmailQueue:
        pass

# Mod√®les Pydantic
class EmailInput(BaseModel):
    texte: str
    expediteur: str = "API_direct"
    destinataire: str = "API_direct"
    objet: str = "Email via API"
    date_reception: str = datetime.now().strftime("%Y-%m-%d")
    departement: str = None

class EmailSimple(BaseModel):
    email: str
    use_cache: bool = True
    rapid_mode: bool = False
    secure_mode: bool = True
    batch_mode: bool = False
    custom_prompt: str = None

# üéØ NOUVEAUX MOD√àLES POUR LES R√âUNIONS
class Participant(BaseModel):
    nom: str
    email: str
    role: str
    present: bool = True
    excuse: str = ""

class Organisateur(BaseModel):
    nom: str
    email: str
    role: str

class FichierAssocie(BaseModel):
    nom: str
    chemin: str
    type: str

class MeetingInput(BaseModel):
    titre: str
    date_reunion: str
    heure_debut: str
    heure_fin: str
    duree_minutes: int
    lieu: str
    organisateur: Organisateur
    participants: List[Participant]
    ordre_du_jour: List[str]
    transcription: str
    departement: str
    projet_associe: str = ""
    priorite_meeting: str = "normale"
    type_reunion: str = "general"
    tags: List[str] = []
    fichiers_associes: List[FichierAssocie] = []

class TranscriptionSimple(BaseModel):
    transcription: str
    meeting_title: str = "R√©union sans titre"
    meeting_date: str = datetime.now().strftime("%Y-%m-%d")
    participants: List[str] = []

# üÜï NOUVEAUX MOD√àLES POUR MODIFICATIONS DE T√ÇCHES - PHASE 6
from typing import Optional
from pydantic import Field, validator

class TaskUpdateComplete(BaseModel):
    """Mod√®le pour modification compl√®te d'une t√¢che (PUT)"""
    description: str = Field(..., min_length=3, max_length=500)
    responsable: str = Field(..., min_length=1, max_length=100)
    priorite: str = Field(..., pattern="^(high|medium|low|urgent)$")
    statut: str = Field(..., pattern="^(pending|in_progress|completed|cancelled|rejected)$")
    deadline: Optional[str] = Field(None, pattern=r"^(\d{4}-\d{2}-\d{2})?$")
    department: Optional[str] = Field(None, max_length=50)
    
    @validator('deadline')
    def validate_deadline(cls, v):
        if v and v.strip():
            try:
                datetime.strptime(v, '%Y-%m-%d')
                return v
            except ValueError:
                raise ValueError('Format de date invalide, utilisez YYYY-MM-DD')
        return None

class TaskComment(BaseModel):
    """Mod√®le pour ajouter un commentaire √† une t√¢che"""
    comment: str = Field(..., min_length=1, max_length=1000)
    author: Optional[str] = Field("user", max_length=100)

# =====================================
# üÜï MOD√àLES POUR ENVOI D'EMAILS
# =====================================

class EmailSendRequest(BaseModel):
    """Mod√®le pour envoyer un email"""
    to_email: str = Field(..., description="Adresse email du destinataire")
    subject: str = Field(..., min_length=1, max_length=200, description="Sujet de l'email")
    body: str = Field(..., min_length=1, max_length=10000, description="Contenu de l'email")
    cc: Optional[List[str]] = Field(None, description="Adresses email en copie")
    bcc: Optional[List[str]] = Field(None, description="Adresses email en copie cach√©e")

    @validator('to_email', 'cc', 'bcc')
    def validate_email_format(cls, v):
        if v is None:
            return v
        if isinstance(v, str):
            # Validation basique du format email
            import re
            email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
            if not re.match(email_pattern, v):
                raise ValueError(f'Format d\'email invalide: {v}')
            return v
        elif isinstance(v, list):
            for email in v:
                import re
                email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
                if not re.match(email_pattern, email):
                    raise ValueError(f'Format d\'email invalide: {email}')
            return v
        return v

class EmailResponse(BaseModel):
    """Mod√®le de r√©ponse pour l'envoi d'email"""
    status: str
    message: str
    email_id: Optional[str] = None
    sent_at: str
    recipient: str
    subject: str

# üè∑Ô∏è NOUVEAUX MOD√àLES POUR SYST√àME DE TAGS - PHASE 7
class TaskTags(BaseModel):
    """Mod√®le pour ajouter des tags √† une t√¢che"""
    tags: List[str] = Field(..., min_items=1, max_items=7)
    
    @validator('tags', each_item=True)
    def validate_tag(cls, v):
        # Normalisation et validation des tags
        if not v or not isinstance(v, str):
            raise ValueError('Tag invalide')
        
        # Normaliser : lowercase, strip, remplacer espaces par tirets
        normalized = v.strip().lower().replace(' ', '-').replace('_', '-')
        
        # Remplacer caract√®res accentu√©s
        import unicodedata
        normalized = unicodedata.normalize('NFD', normalized)
        normalized = ''.join(char for char in normalized if unicodedata.category(char) != 'Mn')
        
        # Validation : alphanum√©riques et tirets seulement
        import re
        if not re.match(r'^[a-z0-9-]+$', normalized):
            raise ValueError(f'Tag invalide: "{v}". Utilisez uniquement lettres, chiffres et tirets')
        
        # Longueur limit√©e
        if len(normalized) < 2 or len(normalized) > 20:
            raise ValueError(f'Tag trop court/long: "{v}". Entre 2 et 20 caract√®res')
        
        return normalized
    
    @validator('tags')
    def validate_unique_tags(cls, v):
        # √âliminer les doublons
        unique_tags = list(set(v))
        if len(unique_tags) != len(v):
            raise ValueError('Tags dupliqu√©s d√©tect√©s')
        return unique_tags

class TaskCreate(BaseModel):
    """
    ‚ú® Mod√®le pour cr√©er une nouvelle t√¢che avec tags
    """
    title: str
    description: str
    priority: str = "medium"  # low, medium, high
    deadline: Optional[str] = None
    department: str = "general"
    tags: Optional[List[str]] = []
    
    @validator('deadline')
    def validate_deadline(cls, v):
        if v:
            try:
                datetime.fromisoformat(v.replace('Z', '+00:00'))
                return v
            except ValueError:
                raise ValueError('Format de date invalide. Utilisez ISO format (YYYY-MM-DDTHH:MM:SS)')
        return v
    
    @validator('tags', each_item=True)
    def normalize_tags(cls, v):
        if not v:
            return v
        
        # Normaliser : lowercase, strip, remplacer espaces par tirets
        normalized = v.strip().lower().replace(' ', '-').replace('_', '-')
        
        # Remplacer caract√®res accentu√©s
        import unicodedata
        normalized = unicodedata.normalize('NFD', normalized)
        normalized = ''.join(char for char in normalized if unicodedata.category(char) != 'Mn')
        
        # Validation : alphanum√©riques et tirets seulement
        import re
        if not re.match(r'^[a-z0-9-]+$', normalized):
            raise ValueError(f'Tag invalide: "{v}". Utilisez uniquement lettres, chiffres et tirets')
        
        # Longueur limit√©e
        if len(normalized) < 2 or len(normalized) > 20:
            raise ValueError(f'Tag trop court/long: "{v}". Entre 2 et 20 caract√®res')
        
        return normalized
    
    @validator('tags')
    def validate_tag_limits(cls, v):
        if len(v) > 7:
            raise ValueError('Maximum 7 tags par t√¢che')
        # √âliminer les doublons
        unique_tags = list(set(v))
        if len(unique_tags) != len(v):
            raise ValueError('Tags dupliqu√©s d√©tect√©s')
        return unique_tags

# üîß MOD√àLES POUR MODIFICATIONS PARTIELLES
class TaskPriorityUpdate(BaseModel):
    """Mod√®le pour modification de priorit√©"""
    priority: str = Field(..., pattern="^(high|medium|low|urgent)$")

class TaskDescriptionUpdate(BaseModel):
    """Mod√®le pour modification de description"""
    description: str = Field(..., min_length=3, max_length=500)

class TaskDeadlineUpdate(BaseModel):
    """Mod√®le pour modification de deadline"""
    deadline: Optional[str] = Field(None, pattern=r"^(\d{4}-\d{2}-\d{2})?$")
    
    @validator('deadline')
    def validate_deadline(cls, v):
        if v and v.strip():
            try:
                datetime.strptime(v, '%Y-%m-%d')
                return v
            except ValueError:
                raise ValueError('Format de date invalide, utilisez YYYY-MM-DD')
        return None

class TaskDepartmentUpdate(BaseModel):
    """Mod√®le pour modification de d√©partement"""
    department: Optional[str] = Field(None, max_length=50)

# Configuration des fichiers
DATA_DIR = "data"
os.makedirs(DATA_DIR, exist_ok=True)

DATA_FILE = os.path.join(DATA_DIR, "tasks.json")
LOG_FILE = os.path.join(DATA_DIR, "logs.json")
UNIFIED_TASKS_FILE = os.path.join(DATA_DIR, "unified_tasks.json")

# üîÑ NOUVEAU: Support du syst√®me unifi√© - PHASE 2
try:
    sys.path.append("src/utils")
    from unified_task_manager import get_unified_task_manager
    UNIFIED_SYSTEM_AVAILABLE = True
    print("‚úÖ Syst√®me unifi√© disponible")
except ImportError:
    UNIFIED_SYSTEM_AVAILABLE = False
    print("‚ö†Ô∏è Syst√®me unifi√© non disponible, utilisation du syst√®me legacy")

# Cr√©er les fichiers s'ils n'existent pas
for file_path in [DATA_FILE, LOG_FILE]:
    if not os.path.exists(file_path):
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump([], f, ensure_ascii=False, indent=4)

# Gestionnaire de cycle de vie
@asynccontextmanager
async def lifespan(app: FastAPI):
    """Gestion du cycle de vie de l'application"""
    print("üöÄ D√©marrage de l'application AI Task Extraction...")
    
    try:
        service = get_background_service()
        if service.start_service():
            print("‚úÖ Service de surveillance temps r√©el d√©marr√©")
        else:
            print("‚ùå Erreur lors du d√©marrage du service de surveillance")
    except Exception as e:
        print(f"‚ö†Ô∏è  Service de surveillance non disponible: {e}")
    
    yield
    
    print("üõë Arr√™t de l'application...")
    try:
        service = get_background_service()
        service.stop_service()
        print("‚úÖ Service de surveillance arr√™t√©")
    except:
        pass

# Application FastAPI
app = FastAPI(
    title="AI Task Extraction System",
    description="Syst√®me intelligent d'extraction de t√¢ches depuis les emails avec toutes les am√©liorations",
    version="2.0.0",
    lifespan=lifespan
)

# Configuration CORS pour permettre les requ√™tes depuis le frontend
from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://127.0.0.1:5173", "http://localhost:5174", "http://localhost:5175", "http://127.0.0.1:5175"],  # Frontend React
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Fonctions utilitaires
def est_doublon(nouvelle_tache, anciennes_taches):
    """V√©rifie si une t√¢che est d√©j√† pr√©sente"""
    for t in anciennes_taches:
        if (t["description"].strip().lower() == nouvelle_tache["description"].strip().lower() and
            t["responsable"].strip().lower() == nouvelle_tache["responsable"].strip().lower()):
            return True
    return False

def ecrire_log(email_objet, statut, resultat_ia, nb_taches=0, erreur=None):
    """√âcrire un log d'√©v√©nement"""
    horodatage = datetime.now().isoformat(timespec='seconds')
    log_entree = {
        "horodatage": horodatage,
        "email_objet": email_objet,
        "statut": statut,
        "resultat_ia_brut": resultat_ia
    }
    if statut == "succ√®s":
        log_entree["taches_extraites"] = nb_taches
    if erreur:
        log_entree["erreur"] = erreur

    try:
        with open(LOG_FILE, "r", encoding="utf-8") as f:
            logs = json.load(f)
    except:
        logs = []

    logs.append(log_entree)

    with open(LOG_FILE, "w", encoding="utf-8") as f:
        json.dump(logs, f, indent=4, ensure_ascii=False)

# =====================================
# ENDPOINTS PRINCIPAUX
# =====================================

@app.get("/")
async def root():
    """Endpoint racine avec informations syst√®me"""
    return {
        "message": "üöÄ AI Task Extraction System - Production Ready",
        "version": "2.0.0",
        "status": "running",
        "features": [
            "Cache anti-doublon",
            "Batch processing", 
            "Rate limiting + Queue",
            "Optimisation prompts IA",
            "Monitoring temps r√©el"
        ],
        "endpoints": {
            "health": "/health",
            "docs": "/docs",
            "emails_explicite": "/email-explicite",
            "emails_implicite": "/email-implicite", 
            "traitement_unifie": "/traiter-emails",
            "monitoring": "/monitoring/system_health",
            "cache": "/cache/stats"
        },
        "uvicorn_ready": True
    }

@app.get("/health")
async def health():
    """Endpoint de sant√© simple"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "project_status": "organized_and_functional"
    }

# =====================================
# ENDPOINTS TRAITEMENT EMAILS
# =====================================

@app.post("/email-explicite")
def handle_email_explicit(input: EmailInput):
    """Traitement d'email avec t√¢ches explicites"""
    try:
        # V√©rification cache
        email_hash = calculer_hash_email(input.texte, input.objet)
        
        if est_email_deja_traite(email_hash):
            info_cache = obtenir_info_cache(email_hash)
            ecrire_log(input.objet, "cache_hit", "R√©cup√©r√© depuis cache", 0)
            
            return {
                "status": "cache_hit",
                "message": "Email similaire d√©j√† trait√©",
                "hash_email": email_hash,
                "info_traitement_precedent": info_cache,
                "nouvelles_taches": [],
                "economies_ia": "Appel OpenRouter √©vit√©"
            }
        
        # Traitement IA
        result_text = extract_tasks_from_email(input.texte)
        tasks = json.loads(result_text)
        
        if not isinstance(tasks, list):
            raise ValueError("R√©sultat IA n'est pas une liste")
        
        # Enrichissement des donn√©es
        resume = resume_email(input.texte)
        if input.departement:
            departement_info = {"nom": input.departement, "origine": "Utilisateur"}
        else:
            nom_dept = identifier_departement(input.texte)
            departement_info = {"nom": nom_dept, "origine": "AI"}

        origine_email = {
            "expediteur": input.expediteur,
            "destinataire": input.destinataire,
            "objet": input.objet,
            "date_reception": input.date_reception,
            "resume_contenu": resume,
            "departement": departement_info
        }

        # Lecture des t√¢ches existantes
        with open(DATA_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)

        nouvelles_taches_ajoutees = []

        for task in tasks:
            # Enrichissement des t√¢ches
            if not task.get("deadline"):
                task["deadline"] = "inconnue"
            if not task.get("responsable"):
                task["responsable"] = "non pr√©cis√©"
            if not task.get("priorite") or task["priorite"].strip() == "":
                task["priorite"] = deduire_priorite(task["description"])

            task["id"] = str(uuid.uuid4())
            task["confiance_ia"] = 1.0
            task["source"] = "email"
            task["type"] = "explicite"
            task["extrait_le"] = datetime.now().isoformat(timespec='seconds')
            task["statut"] = "√† faire"
            task["origine_email"] = origine_email

            # V√©rification anti-doublon
            if est_doublon(task, data):
                print(f"üü° Doublon d√©tect√© ‚Äî t√¢che ignor√©e : {task['description']}")
                continue
            else:
                data.append(task)
                nouvelles_taches_ajoutees.append(task)

        # ‚úÖ NOUVELLE LOGIQUE: Sauvegarde dans syst√®me unifi√©
        if UNIFIED_SYSTEM_AVAILABLE:
            # Mode moderne : Sauvegarder dans syst√®me unifi√©
            unified_manager = get_unified_task_manager()
            
            for task in nouvelles_taches_ajoutees:
                # Convertir format legacy vers unifi√©
                unified_task_data = {
                    "description": task["description"],
                    "responsable": task["responsable"],
                    "deadline": task.get("deadline"),
                    "priorite": task["priorite"],
                    "statut": task["statut"],
                    "confiance_ia": task["confiance_ia"],
                    "source": "email",
                    "type": task["type"],
                    "source_metadata": {
                        "original_email": task["origine_email"],
                        "email_id": task["id"],
                        "extraction_timestamp": task["extrait_le"]
                    }
                }
                
                # Ajouter au syst√®me unifi√©
                unified_task_id = unified_manager.add_task(unified_task_data)
                print(f"‚úÖ T√¢che ajout√©e au syst√®me unifi√©: {unified_task_id}")
        
        # Sauvegarde LEGACY (pour compatibilit√©)
        with open(DATA_FILE, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=4, ensure_ascii=False)

        ecrire_log(input.objet, "succ√®s", result_text, len(nouvelles_taches_ajoutees))
        
        # Marquer email comme trait√© dans le cache
        marquer_email_traite(email_hash, {
            "objet": input.objet,
            "expediteur": input.expediteur,
            "destinataire": input.destinataire,
            "nb_taches": len(nouvelles_taches_ajoutees),
            "type_email": "explicite"
        })

        return {
            "status": "success",
            "ajoutees": len(nouvelles_taches_ajoutees),
            "taches": nouvelles_taches_ajoutees,
            "hash_email": email_hash,
            "cache_status": "nouvel_email_ajoute_au_cache"
        }
        
    except Exception as e:
        ecrire_log(input.objet, "√©chec", str(e), 0, str(e))
        raise HTTPException(status_code=500, detail=f"Erreur traitement: {str(e)}")

@app.post("/email-implicite")
def handle_email_implicite(input: EmailInput):
    """Traitement d'email avec t√¢ches implicites"""
    try:
        # V√©rification cache
        email_hash = calculer_hash_email(input.texte, input.objet)
        
        if est_email_deja_traite(email_hash):
            info_cache = obtenir_info_cache(email_hash)
            ecrire_log(input.objet, "cache_hit", "R√©cup√©r√© depuis cache", 0)
            
            return {
                "status": "cache_hit",
                "message": "Email similaire d√©j√† trait√©",
                "hash_email": email_hash,
                "info_traitement_precedent": info_cache,
                "nouvelles_taches": [],
                "economies_ia": "Appel OpenRouter √©vit√©"
            }
        
        # Traitement IA pour t√¢ches implicites
        result_text = suggere_taches_implicites(input.texte)
        tasks = json.loads(result_text)
        
        if not isinstance(tasks, list):
            raise ValueError("R√©sultat IA n'est pas une liste")
        
        # Enrichissement des donn√©es
        resume = resume_email(input.texte)
        if input.departement:
            departement_info = {"nom": input.departement, "origine": "Utilisateur"}
        else:
            nom_dept = identifier_departement(input.texte)
            departement_info = {"nom": nom_dept, "origine": "AI"}

        origine_email = {
            "expediteur": input.expediteur,
            "destinataire": input.destinataire,
            "objet": input.objet,
            "date_reception": input.date_reception,
            "resume_contenu": resume,
            "departement": departement_info
        }

        with open(DATA_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)

        nouvelles_taches_ajoutees = []

        for task in tasks:
            if not task.get("responsable"):
                task["responsable"] = "inconnu"
            if not task.get("priorite"):
                task["priorite"] = deduire_priorite(task["description"])
            if not task.get("confiance_ia"):
                task["confiance_ia"] = 0.6

            task["id"] = str(uuid.uuid4())
            task["source"] = "email"
            task["type"] = "implicite"
            task["extrait_le"] = datetime.now().isoformat(timespec='seconds')
            task["statut"] = "√† faire"
            task["origine_email"] = origine_email

            if est_doublon(task, data):
                print(f"üü° Doublon d√©tect√© ‚Äî t√¢che ignor√©e : {task['description']}")
                continue
            else:
                data.append(task)
                nouvelles_taches_ajoutees.append(task)

        # ‚úÖ NOUVELLE LOGIQUE: Sauvegarde dans syst√®me unifi√© (email-implicite)
        if UNIFIED_SYSTEM_AVAILABLE:
            # Mode moderne : Sauvegarder dans syst√®me unifi√©
            unified_manager = get_unified_task_manager()
            
            for task in nouvelles_taches_ajoutees:
                # Convertir format legacy vers unifi√©
                unified_task_data = {
                    "description": task["description"],
                    "responsable": task["responsable"],
                    "deadline": task.get("deadline"),
                    "priorite": task["priorite"],
                    "statut": task["statut"],
                    "confiance_ia": task["confiance_ia"],
                    "source": "email",
                    "type": task["type"],
                    "source_metadata": {
                        "original_email": task["origine_email"],
                        "email_id": task["id"],
                        "extraction_timestamp": task["extrait_le"]
                    }
                }
                
                # Ajouter au syst√®me unifi√©
                unified_task_id = unified_manager.add_task(unified_task_data)
                print(f"‚úÖ T√¢che ajout√©e au syst√®me unifi√©: {unified_task_id}")

        # Sauvegarde LEGACY (pour compatibilit√©)
        with open(DATA_FILE, "w", encoding="utf-8") as f:
            json.dump(data, f, indent=4, ensure_ascii=False)

        ecrire_log(input.objet, "succ√®s", result_text, len(nouvelles_taches_ajoutees))
        
        marquer_email_traite(email_hash, {
            "objet": input.objet,
            "expediteur": input.expediteur,
            "destinataire": input.destinataire,
            "nb_taches": len(nouvelles_taches_ajoutees),
            "type_email": "implicite"
        })

        return {
            "status": "success",
            "ajoutees": len(nouvelles_taches_ajoutees),
            "taches": nouvelles_taches_ajoutees,
            "hash_email": email_hash,
            "cache_status": "nouvel_email_ajoute_au_cache"
        }
        
    except Exception as e:
        ecrire_log(input.objet, "√©chec", str(e), 0, str(e))
        raise HTTPException(status_code=500, detail=f"Erreur traitement: {str(e)}")

@app.get("/traiter-emails")
def api_traiter_emails_unifie(
    use_rate_limiting: bool = False,
    use_batch_processing: bool = True,
    use_cache: bool = True,
    use_optimized_prompts: bool = True
    ):
    """üöÄ ENDPOINT UNIFI√â: Traitement emails avec toutes les am√©liorations"""
    try:
        resultat = traiter_emails(
            use_rate_limiting=use_rate_limiting,
            use_batch_processing=use_batch_processing,
            use_cache=use_cache,
            use_optimized_prompts=use_optimized_prompts
        )
        
        optimisations_actives = []
        if use_cache:
            optimisations_actives.append("Cache anti-doublon")
        if use_batch_processing:
            optimisations_actives.append("Batch processing intelligent")
        if use_rate_limiting:
            optimisations_actives.append("Rate limiting + Queue")
        if use_optimized_prompts:
            optimisations_actives.append("Prompts IA optimis√©s")
        
        return {
            "status": "success",
            "mode": "production_optimized_processing",
            "optimisations_actives": optimisations_actives,
            "details": resultat,
            "message": f"Emails trait√©s avec {len(optimisations_actives)} optimisations actives"
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur traitement unifi√©: {str(e)}")

@app.post("/traiter_emails")
def traiter_emails_post(input: EmailSimple):
    """Endpoint POST pour traitement d'email simple"""
    try:
        # Convertir vers EmailInput
        email_input = EmailInput(
            texte=input.email,
            objet="Email via API POST"
        )
        
        # Utiliser la logique explicite
        return handle_email_explicit(email_input)
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur traitement: {str(e)}")

# =====================================
# ENDPOINTS MONITORING
# =====================================

@app.get("/monitoring/system_health")
def system_health():
    """Sant√© globale du syst√®me"""
    try:
        service = get_background_service()
        status = service.get_service_status()
    except:
        status = {"service_running": False, "error": "Service non disponible"}
    
    files_status = {}
    for file_name in ["data/emails.json", "data/tasks.json", "data/logs.json"]:
        files_status[file_name] = {
            "exists": os.path.exists(file_name),
            "readable": os.access(file_name, os.R_OK) if os.path.exists(file_name) else False,
            "writable": os.access(file_name, os.W_OK) if os.path.exists(file_name) else False
        }
    
    return {
        "system_status": "healthy",
        "service_status": status,
        "files_status": files_status,
        "timestamp": datetime.now().isoformat()
    }

@app.get("/monitoring/rate_limit")
def get_rate_limit_status():
    """Statut du rate limiter"""
    try:
        limiter = RateLimiter()
        stats = limiter.get_current_stats()
        return {
            "status": "success",
            "rate_limit_stats": stats,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"Rate limiter non disponible: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }

@app.get("/monitoring/queue")
def get_queue_status():
    """Statut de la file d'attente"""
    return {
        "status": "success",
        "message": "Queue status endpoint disponible",
        "note": "Queue cr√©√©e dynamiquement lors du traitement rate-limited",
        "timestamp": datetime.now().isoformat()
    }

# =====================================
# ENDPOINTS CACHE
# =====================================

@app.get("/cache/stats")
def get_cache_statistics():
    """Statistiques du cache d'emails"""
    try:
        stats = obtenir_statistiques_cache()
        stats["cache_file_exists"] = os.path.exists("data/emails_cache.json")
        if stats["cache_file_exists"]:
            stats["cache_file_size"] = os.path.getsize("data/emails_cache.json")
        
        return {
            "status": "success",
            "cache_statistics": stats,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"Erreur cache: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }

@app.post("/cache/cleanup")
def cleanup_cache(retention_days: int = 30):
    """Nettoyage du cache"""
    try:
        nettoyer_cache_ancien(retention_days)
        stats_apres = obtenir_statistiques_cache()
        
        return {
            "status": "success",
            "message": f"Cache nettoy√© - emails plus anciens que {retention_days} jours supprim√©s",
            "stats_apres_nettoyage": stats_apres,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"Erreur nettoyage: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }

@app.delete("/cache/clear")
def clear_cache():
    """Vider compl√®tement le cache"""
    try:
        cache_file = "data/emails_cache.json"
        if os.path.exists(cache_file):
            with open(cache_file, "w", encoding="utf-8") as f:
                json.dump({}, f)
        
        return {
            "status": "success",
            "message": "Cache vid√© avec succ√®s",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        return {
            "status": "error", 
            "message": f"Erreur vidage cache: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }

# =====================================
# ENDPOINTS SYST√àME
# =====================================

def normalize_filter_value(field, value):
    """
    üîß Normalise les valeurs de filtre pour accepter les variantes
    Solution simple pour filtrage intelligent
    """
    if not value:
        return ""
    
    value_lower = value.lower().strip()
    
    # Mappings pour filtrage intelligent
    STATUS_MAPPING = {
        "√† faire": "pending",
        "a faire": "pending", 
        "en cours": "in_progress",
        "en_cours": "in_progress",
        "termin√©": "completed",
        "termine": "completed",
        "fini": "completed",
        "annul√©": "cancelled",
        "annule": "cancelled"
    }
    
    PRIORITY_MAPPING = {
        "haute": "high",
        "elev√©": "high", 
        "√©lev√©": "high",
        "urgent": "high",
        "moyenne": "medium",
        "normale": "medium",
        "normal": "medium", 
        "moyen": "medium",
        "critique": "critical",
        "prioritaire": "critical",
        "basse": "low",
        "faible": "low",
        "bas": "low"
    }
    
    if field == "status":
        return STATUS_MAPPING.get(value_lower, value_lower)
    elif field == "priority":
        return PRIORITY_MAPPING.get(value_lower, value_lower)
    else:
        return value_lower

def smart_filter_match(task_value, filter_value, field_type):
    """
    üß† Comparaison intelligente pour le filtrage
    Accepte les variantes de valeurs
    """
    if not task_value or not filter_value:
        return False
    
    # Normaliser les deux valeurs
    normalized_task = normalize_filter_value(field_type, task_value)
    normalized_filter = normalize_filter_value(field_type, filter_value)
    
    return normalized_task == normalized_filter

# =====================================
# üéØ PHASE 2 - FONCTIONS DE FILTRAGE AVANC√â
# =====================================

def parse_date_string(date_str):
    """
    üìÖ Parse une date en format YYYY-MM-DD ou ISO
    Retourne None si parsing √©choue
    """
    if not date_str or date_str == "null":
        return None
    
    try:
        # Essayer format YYYY-MM-DD
        if len(date_str) == 10 and date_str.count('-') == 2:
            return datetime.strptime(date_str, "%Y-%m-%d")
        
        # Essayer format ISO complet
        if 'T' in date_str:
            # Enlever les microsecondes si pr√©sentes
            if '.' in date_str:
                date_str = date_str.split('.')[0]
            return datetime.fromisoformat(date_str.replace('Z', ''))
        
        return None
    except:
        return None

def filter_by_date_range(tasks, deadline_before=None, deadline_after=None, created_after=None, created_before=None):
    """
    üìÖ Filtrer les t√¢ches par plages de dates
    """
    if not (deadline_before or deadline_after or created_after or created_before):
        return tasks
    
    filtered_tasks = []
    
    for task in tasks:
        include_task = True
        
        # Filtres deadline
        if deadline_before or deadline_after:
            task_deadline = parse_date_string(task.get('deadline'))
            
            if deadline_before:
                deadline_before_date = parse_date_string(deadline_before)
                if deadline_before_date and task_deadline:
                    if task_deadline >= deadline_before_date:
                        include_task = False
                elif deadline_before_date and not task_deadline:
                    # Si filter deadline_before mais task sans deadline, inclure
                    pass
            
            if deadline_after and include_task:
                deadline_after_date = parse_date_string(deadline_after)
                if deadline_after_date and task_deadline:
                    if task_deadline <= deadline_after_date:
                        include_task = False
                elif deadline_after_date and not task_deadline:
                    # Si filter deadline_after mais task sans deadline, exclure
                    include_task = False
        
        # Filtres created_at
        if (created_after or created_before) and include_task:
            task_created = parse_date_string(task.get('created_at'))
            
            if created_after:
                created_after_date = parse_date_string(created_after)
                if created_after_date and task_created:
                    if task_created <= created_after_date:
                        include_task = False
            
            if created_before and include_task:
                created_before_date = parse_date_string(created_before)
                if created_before_date and task_created:
                    if task_created >= created_before_date:
                        include_task = False
        
        if include_task:
            filtered_tasks.append(task)
    
    return filtered_tasks

def filter_by_source(tasks, source):
    """
    üì® Filtrer les t√¢ches par source (email, meeting)
    """
    if not source:
        return tasks
    
    source_lower = source.lower().strip()
    return [task for task in tasks if task.get('source', '').lower() == source_lower]

def extract_department_from_task(task):
    """
    üè¢ Extraire le d√©partement d'une t√¢che depuis les m√©tadonn√©es
    """
    metadata = task.get('source_metadata', {})
    
    # Pour les emails
    if 'original_email' in metadata:
        email_data = metadata['original_email']
        dept = email_data.get('departement')
        
        if isinstance(dept, dict) and 'nom' in dept:
            return dept['nom']
        elif isinstance(dept, str):
            return dept
    
    # Pour les meetings  
    if 'meeting' in metadata or task.get('origine_meeting'):
        meeting_data = task.get('origine_meeting', {})
        dept = meeting_data.get('departement')
        if dept:
            return dept
    
    return None

def filter_by_department(tasks, department):
    """
    üè¢ Filtrer les t√¢ches par d√©partement
    """
    if not department:
        return tasks
    
    department_lower = department.lower().strip()
    filtered_tasks = []
    
    for task in tasks:
        task_dept = extract_department_from_task(task)
        if task_dept and department_lower in task_dept.lower():
            filtered_tasks.append(task)
    
    return filtered_tasks

# =====================================
# üîç PHASE 3 - FONCTIONS DE RECHERCHE TEXTUELLE
# =====================================

import re
import unicodedata

def normalize_text_for_search(text):
    """
    üî§ Normalise le texte pour la recherche (accents, casse, etc.)
    """
    if not text:
        return ""
    
    # Convertir en minuscules
    text = text.lower()
    
    # Supprimer les accents
    text = unicodedata.normalize('NFD', text)
    text = ''.join(char for char in text if unicodedata.category(char) != 'Mn')
    
    # Nettoyer les caract√®res sp√©ciaux (garder espaces, lettres, chiffres)
    text = re.sub(r'[^\w\s]', ' ', text)
    
    # Normaliser les espaces
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def search_in_text(text, query):
    """
    üîç Recherche simple dans un texte
    Retourne True si le query est trouv√©
    """
    if not text or not query:
        return False
    
    normalized_text = normalize_text_for_search(text)
    normalized_query = normalize_text_for_search(query)
    
    return normalized_query in normalized_text

def search_in_task_fields(task, query, search_fields=['description']):
    """
    üîç Recherche dans les champs sp√©cifi√©s d'une t√¢che
    
    search_fields options:
    - ['description'] : recherche uniquement dans description
    - ['responsable'] : recherche uniquement dans responsable  
    - ['description', 'responsable'] : recherche dans les deux
    - ['all'] : recherche dans tous les champs textuels
    """
    if not query:
        return False
    
    # D√©finir tous les champs de recherche possibles
    all_searchable_fields = {
        'description': task.get('description', ''),
        'responsable': task.get('responsable', ''),
        'statut': task.get('statut', ''),
        'priorite': task.get('priorite', ''),
        'type': task.get('type', ''),
        'deadline': str(task.get('deadline', ''))
    }
    
    # Ajouter m√©tadonn√©es emails si disponibles
    metadata = task.get('source_metadata', {})
    if 'original_email' in metadata:
        email_data = metadata['original_email']
        if isinstance(email_data, dict):
            all_searchable_fields.update({
                'email_objet': email_data.get('objet', ''),
                'email_expediteur': email_data.get('expediteur', ''),
                'email_destinataire': email_data.get('destinataire', ''),
                'email_resume': email_data.get('resume_contenu', '')
            })
    
    # D√©terminer les champs √† rechercher
    if 'all' in search_fields:
        fields_to_search = all_searchable_fields
    else:
        fields_to_search = {k: v for k, v in all_searchable_fields.items() if k in search_fields}
    
    # Rechercher dans chaque champ
    for field_name, field_value in fields_to_search.items():
        if search_in_text(field_value, query):
            return True
    
    return False

def filter_by_search(tasks, search_query, search_in=['description']):
    """
    üîç Filtrer les t√¢ches par recherche textuelle
    
    Args:
        tasks: Liste des t√¢ches
        search_query: Terme de recherche
        search_in: Liste des champs o√π chercher
    
    Returns:
        Liste des t√¢ches correspondantes
    """
    if not search_query:
        return tasks
    
    filtered_tasks = []
    for task in tasks:
        if search_in_task_fields(task, search_query, search_in):
            filtered_tasks.append(task)
    
    return filtered_tasks

def get_search_highlights(task, query, max_length=100):
    """
    üîç Obtenir des extraits avec mise en √©vidence des termes recherch√©s
    """
    if not query:
        return ""
    
    description = task.get('description', '')
    if not description:
        return ""
    
    normalized_desc = normalize_text_for_search(description)
    normalized_query = normalize_text_for_search(query)
    
    # Trouver la position du terme
    pos = normalized_desc.find(normalized_query)
    if pos == -1:
        return description[:max_length] + "..." if len(description) > max_length else description
    
    # Cr√©er un extrait autour du terme trouv√©
    start = max(0, pos - 30)
    end = min(len(description), pos + len(query) + 30)
    
    excerpt = description[start:end]
    if start > 0:
        excerpt = "..." + excerpt
    if end < len(description):
        excerpt = excerpt + "..."
    
    return excerpt

# ==============================================================================
# üìÑ PHASE 4: PAGINATION ET TRI AVANC√â
# ==============================================================================

import math

def sort_tasks(tasks, sort_by=None, order="asc"):
    """
    üìà Trier les t√¢ches par champ sp√©cifi√©
    
    Args:
        tasks: Liste des t√¢ches
        sort_by: Champ de tri (priority, deadline, created_at, status, responsable, description)
        order: Ordre (asc ou desc)
    
    Returns:
        Liste des t√¢ches tri√©es
    """
    if not sort_by or not tasks:
        return tasks
    
    reverse = (order.lower() == "desc")
    
    try:
        # Tri par priorit√© avec ordre logique
        if sort_by == "priority":
            priority_order = {"high": 3, "medium": 2, "low": 1, "": 0}
            return sorted(tasks, 
                         key=lambda x: priority_order.get(x.get("priorite", "").lower(), 0),
                         reverse=reverse)
        
        # Tri par statut avec ordre logique  
        elif sort_by == "status":
            status_order = {"pending": 4, "in_progress": 3, "completed": 2, "cancelled": 1, "": 0}
            return sorted(tasks,
                         key=lambda x: status_order.get(x.get("statut", "").lower(), 0),
                         reverse=reverse)
        
        # Tri par date (deadline, created_at)
        elif sort_by in ["deadline", "created_at"]:
            field_name = "echeance" if sort_by == "deadline" else "date_creation"
            return sorted(tasks,
                         key=lambda x: x.get(field_name, "") or "9999-12-31",
                         reverse=reverse)
        
        # Tri alphab√©tique (responsable, description)
        elif sort_by in ["responsable", "description"]:
            return sorted(tasks,
                         key=lambda x: str(x.get(sort_by, "")).lower(),
                         reverse=reverse)
        
        # Tri par source
        elif sort_by == "source":
            return sorted(tasks,
                         key=lambda x: x.get("source", ""),
                         reverse=reverse)
        
        # Champ non support√©
        else:
            return tasks
            
    except Exception as e:
        print(f"Erreur tri: {e}")
        return tasks

def sort_by_relevance(tasks, search_term):
    """
    üéØ Trier les t√¢ches par pertinence de recherche
    
    Args:
        tasks: Liste des t√¢ches
        search_term: Terme de recherche pour calculer la pertinence
    
    Returns:
        Liste des t√¢ches tri√©es par pertinence (plus pertinentes d'abord)
    """
    if not search_term or not tasks:
        return tasks
    
    def calculate_relevance_score(task):
        """Calculer le score de pertinence d'une t√¢che"""
        score = 0
        search_normalized = normalize_text_for_search(search_term)
        
        # Score description (poids 5 - plus important)
        description = normalize_text_for_search(task.get("description", ""))
        if search_normalized in description:
            score += 5
            # Bonus si le terme est au d√©but
            if description.startswith(search_normalized):
                score += 2
        
        # Score responsable (poids 3)
        responsable = normalize_text_for_search(task.get("responsable", ""))
        if search_normalized in responsable:
            score += 3
        
        # Score autres champs (poids 1 chacun)
        for field in ["source", "department", "priorite", "statut"]:
            field_value = normalize_text_for_search(str(task.get(field, "")))
            if search_normalized in field_value:
                score += 1
        
        # Score m√©tadonn√©es email (poids 2)
        metadata = task.get('source_metadata', {})
        if 'original_email' in metadata:
            email_data = metadata['original_email']
            if isinstance(email_data, dict):
                for email_field in ['objet', 'expediteur', 'resume_contenu']:
                    email_value = normalize_text_for_search(str(email_data.get(email_field, "")))
                    if search_normalized in email_value:
                        score += 2
        
        return score
    
    try:
        return sorted(tasks, key=calculate_relevance_score, reverse=True)
    except Exception as e:
        print(f"Erreur tri pertinence: {e}")
        return tasks

def paginate_tasks(tasks, page=1, limit=20):
    """
    üìÑ Paginer la liste des t√¢ches
    
    Args:
        tasks: Liste des t√¢ches
        page: Num√©ro de page (commence √† 1)
        limit: Nombre d'√©l√©ments par page
    
    Returns:
        Dict avec t√¢ches pagin√©es et m√©tadonn√©es pagination
    """
    if page < 1:
        page = 1
    if limit < 1:
        limit = 20
    if limit > 100:  # Limite de s√©curit√©
        limit = 100
    
    total_tasks = len(tasks)
    total_pages = math.ceil(total_tasks / limit) if total_tasks > 0 else 1
    
    # Calculer les indices de d√©but et fin
    start_index = (page - 1) * limit
    end_index = start_index + limit
    
    # Extraire les t√¢ches de la page courante
    paginated_tasks = tasks[start_index:end_index]
    
    # M√©tadonn√©es de pagination
    pagination_info = {
        "total_tasks": total_tasks,
        "total_pages": total_pages,
        "current_page": page,
        "page_size": limit,
        "has_next": page < total_pages,
        "has_previous": page > 1,
        "next_page": page + 1 if page < total_pages else None,
        "previous_page": page - 1 if page > 1 else None,
        "start_index": start_index + 1 if total_tasks > 0 else 0,
        "end_index": min(end_index, total_tasks) if total_tasks > 0 else 0
    }
    
    return {
        "tasks": paginated_tasks,
        "pagination": pagination_info
    }

def get_available_sort_fields():
    """
    üìã Obtenir la liste des champs disponibles pour le tri
    
    Returns:
        Liste des champs de tri support√©s
    """
    return [
        "priority",      # Tri par priorit√© (high, medium, low)
        "status",        # Tri par statut (pending, in_progress, completed, cancelled)
        "deadline",      # Tri par √©ch√©ance
        "created_at",    # Tri par date de cr√©ation
        "responsable",   # Tri alphab√©tique par responsable
        "description",   # Tri alphab√©tique par description
        "source",        # Tri par source (email, meeting, log)
        "relevance"      # Tri par pertinence (seulement avec search)
    ]

@app.get("/all-tasks")
def get_all_tasks(
    format: str = "unified",
    # Phase 1 - Filtres simples (existants)
    status: str = None,
    priority: str = None,
    assignee: str = None,
    validated: bool = None,         # üÜï NOUVEAU: Filtrer par validation humaine
    # Phase 2 - Filtres avanc√©s (nouveaux)
    deadline_before: str = None,    # Format: YYYY-MM-DD
    deadline_after: str = None,     # Format: YYYY-MM-DD  
    created_after: str = None,      # Format: YYYY-MM-DD
    created_before: str = None,     # Format: YYYY-MM-DD
    source: str = None,             # email ou meeting
    department: str = None,         # Nom du d√©partement
    # Phase 3 - Recherche textuelle (nouveaux)
    search: str = None,             # Terme de recherche
    search_in: str = "description", # Champs de recherche: description, responsable, all
    # Phase 4 - Pagination et tri (nouveaux)
    page: int = 1,                  # Num√©ro de page (commence √† 1)
    limit: int = 20,                # Nombre d'√©l√©ments par page (max 100)
    sort_by: str = None,            # Champ de tri (priority, deadline, status, relevance, etc.)
    order: str = "asc",             # Ordre de tri (asc ou desc)
    # Phase 7 - Filtres par tags (nouveaux) üè∑Ô∏è
    tag: str = None,                # Filtrer par un tag sp√©cifique
    tags: List[str] = Query([])     # Filtrer par plusieurs tags (ET logique)
):
    """
    üéØ PHASE 1-7: R√©cup√©rer toutes les t√¢ches avec filtres, recherche, pagination, tri et tags
    
    Parameters:
    Format & Filtres Phase 1:
    - format: "unified" (d√©faut) ou "legacy" pour compatibilit√©
    - status: Filtrer par statut (pending, completed, in_progress, cancelled)
    - priority: Filtrer par priorit√© (high, medium, low)
    - assignee: Filtrer par responsable (nom ou email)
    - validated: Filtrer par validation humaine (true/false) üÜï
    
    Filtres Phase 2 (avanc√©s):
    - deadline_before: T√¢ches avec deadline avant cette date (YYYY-MM-DD)
    - deadline_after: T√¢ches avec deadline apr√®s cette date (YYYY-MM-DD)
    - created_after: T√¢ches cr√©√©es apr√®s cette date (YYYY-MM-DD) 
    - created_before: T√¢ches cr√©√©es avant cette date (YYYY-MM-DD)
    - source: Filtrer par source (email, meeting)
    - department: Filtrer par d√©partement (Finance, IT, RH, etc.)
    
    Recherche Phase 3 (textuelle):
    - search: Terme de recherche dans les textes
    - search_in: Champs de recherche (description, responsable, all)
    
    Pagination & Tri Phase 4:
    - page: Num√©ro de page (d√©faut: 1)
    - limit: T√¢ches par page (d√©faut: 20, max: 100)
    - sort_by: Champ de tri (priority, deadline, status, responsable, description, source, relevance)
    - order: Ordre de tri (asc ou desc, d√©faut: asc)
    
    Filtres Tags Phase 7 (nouveaux) üè∑Ô∏è:
    - tag: Filtrer par un tag sp√©cifique (ex: "urgent")
    - tags: Filtrer par plusieurs tags avec ET logique (ex: ["urgent", "bug"])
    """
    try:
        # üöÄ SYST√àME UNIFI√â: Format moderne par d√©faut
        if UNIFIED_SYSTEM_AVAILABLE and os.path.exists(UNIFIED_TASKS_FILE):
            unified_manager = get_unified_task_manager()
            
            if format == "legacy":
                # Mode compatibilit√© : format legacy (emails seulement)
                legacy_email_tasks = unified_manager.get_legacy_format_email_tasks()
                
                # üß† FILTRAGE INTELLIGENT PHASE 1: Appliquer filtres sur legacy aussi
                filtered_legacy_tasks = legacy_email_tasks
                total_before_filter = len(legacy_email_tasks)
                
                if status:
                    filtered_legacy_tasks = [t for t in filtered_legacy_tasks if smart_filter_match(t.get('statut', ''), status, 'status')]
                if priority:
                    filtered_legacy_tasks = [t for t in filtered_legacy_tasks if smart_filter_match(t.get('priorite', ''), priority, 'priority')]
                if assignee:
                    filtered_legacy_tasks = [t for t in filtered_legacy_tasks if assignee.lower() in t.get('responsable', '').lower()]
                if validated is not None:
                    filtered_legacy_tasks = [t for t in filtered_legacy_tasks if t.get('validated', False) == validated]
                
                # üè∑Ô∏è FILTRAGE PAR TAGS PHASE 7: Support legacy
                if tag:
                    filtered_legacy_tasks = [t for t in filtered_legacy_tasks if tag in t.get('tags', [])]
                if tags:
                    filtered_legacy_tasks = [t for t in filtered_legacy_tasks if all(tag_filter in t.get('tags', []) for tag_filter in tags)]
                
                # üöÄ FILTRAGE AVANC√â PHASE 2: M√™me pour legacy compatibility
                filtered_legacy_tasks = filter_by_date_range(
                    filtered_legacy_tasks, 
                    deadline_before=deadline_before,
                    deadline_after=deadline_after,
                    created_after=created_after,
                    created_before=created_before
                )
                
                if source:
                    filtered_legacy_tasks = filter_by_source(filtered_legacy_tasks, source)
                
                if department:
                    filtered_legacy_tasks = filter_by_department(filtered_legacy_tasks, department)
                
                # üîç FILTRAGE RECHERCHE PHASE 3: Appliquer recherche textuelle pour legacy
                if search:
                    # Parser les champs de recherche
                    search_fields = []
                    if search_in == "all":
                        search_fields = ["all"]
                    elif search_in == "responsable":
                        search_fields = ["responsable"]
                    else:  # d√©faut: description
                        search_fields = ["description"]
                    
                    filtered_legacy_tasks = filter_by_search(filtered_legacy_tasks, search, search_fields)
                
                # üìÑ PHASE 4: TRI ET PAGINATION pour legacy
                # Appliquer le tri
                if sort_by == "relevance" and search:
                    # Tri par pertinence de recherche
                    filtered_legacy_tasks = sort_by_relevance(filtered_legacy_tasks, search)
                elif sort_by:
                    # Tri standard
                    filtered_legacy_tasks = sort_tasks(filtered_legacy_tasks, sort_by, order)
                
                # Appliquer la pagination
                pagination_result = paginate_tasks(filtered_legacy_tasks, page, limit)
                paginated_tasks = pagination_result["tasks"]
                pagination_info = pagination_result["pagination"]
                
                return {
                    "tasks": paginated_tasks,
                    "total": len(filtered_legacy_tasks),  # Total apr√®s filtres
                    "total_before_filter": total_before_filter,
                    "pagination": pagination_info,
                    "sorting": {
                        "sort_by": sort_by,
                        "order": order,
                        "available_sorts": get_available_sort_fields()
                    },
                    "filters_applied": {
                        # Phase 1
                        "status": status,
                        "priority": priority,
                        "assignee": assignee,
                        # Phase 2
                        "deadline_before": deadline_before,
                        "deadline_after": deadline_after,
                        "created_after": created_after,
                        "created_before": created_before,
                        "source": source,
                        "department": department,
                        # Phase 3
                        "search": search,
                        "search_in": search_in,
                        # Phase 4
                        "page": page,
                        "limit": limit,
                        "sort_by": sort_by,
                        "order": order
                    },
                    "format": "legacy",
                    "system": "unified"
                }
            else:
                # Mode par d√©faut : format unifi√© (emails + meetings) avec filtres intelligents
                unified_tasks = unified_manager.load_all_tasks()
                total_before_filter = len(unified_tasks)
                
                # üß† FILTRAGE INTELLIGENT PHASE 1: Appliquer filtres simples avec support variantes
                filtered_tasks = unified_tasks
                
                if status:
                    filtered_tasks = [t for t in filtered_tasks if smart_filter_match(t.get('statut', ''), status, 'status')]
                if priority:
                    filtered_tasks = [t for t in filtered_tasks if smart_filter_match(t.get('priorite', ''), priority, 'priority')]
                if assignee:
                    filtered_tasks = [t for t in filtered_tasks if assignee.lower() in t.get('responsable', '').lower()]
                if validated is not None:
                    filtered_tasks = [t for t in filtered_tasks if t.get('validated', False) == validated]
                
                # üè∑Ô∏è FILTRAGE PAR TAGS PHASE 7: Mode unifi√©
                if tag:
                    filtered_tasks = [t for t in filtered_tasks if tag in t.get('tags', [])]
                if tags:
                    filtered_tasks = [t for t in filtered_tasks if all(tag_filter in t.get('tags', []) for tag_filter in tags)]
                
                # üöÄ FILTRAGE AVANC√â PHASE 2: Appliquer nouveaux filtres
                filtered_tasks = filter_by_date_range(
                    filtered_tasks, 
                    deadline_before=deadline_before,
                    deadline_after=deadline_after,
                    created_after=created_after,
                    created_before=created_before
                )
                
                if source:
                    filtered_tasks = filter_by_source(filtered_tasks, source)
                
                if department:
                    filtered_tasks = filter_by_department(filtered_tasks, department)
                
                # üîç FILTRAGE RECHERCHE PHASE 3: Appliquer recherche textuelle
                if search:
                    # Parser les champs de recherche
                    search_fields = []
                    if search_in == "all":
                        search_fields = ["all"]
                    elif search_in == "responsable":
                        search_fields = ["responsable"]
                    else:  # d√©faut: description
                        search_fields = ["description"]
                    
                    filtered_tasks = filter_by_search(filtered_tasks, search, search_fields)
                
                # üìÑ PHASE 4: TRI ET PAGINATION pour mode unifi√©
                # Appliquer le tri
                if sort_by == "relevance" and search:
                    # Tri par pertinence de recherche
                    filtered_tasks = sort_by_relevance(filtered_tasks, search)
                elif sort_by:
                    # Tri standard
                    filtered_tasks = sort_tasks(filtered_tasks, sort_by, order)
                
                # Statistiques par source sur les t√¢ches filtr√©es (avant pagination)
                sources = {}
                for task in filtered_tasks:
                    source = task.get('source', 'unknown')
                    sources[source] = sources.get(source, 0) + 1
                
                # Statistiques de filtrage (avant pagination)
                filters_stats = {
                    "total_before_filter": total_before_filter,
                    "total_after_filter": len(filtered_tasks),
                    "filtered_out": total_before_filter - len(filtered_tasks)
                }
                
                # Appliquer la pagination
                pagination_result = paginate_tasks(filtered_tasks, page, limit)
                paginated_tasks = pagination_result["tasks"]
                pagination_info = pagination_result["pagination"]
                
                return {
                    "tasks": paginated_tasks,  # T√¢ches de la page courante
                    "total": len(filtered_tasks),  # Total apr√®s filtres mais avant pagination
                    "pagination": pagination_info,
                    "sorting": {
                        "sort_by": sort_by,
                        "order": order,
                        "available_sorts": get_available_sort_fields()
                    },
                    "statistics": {
                        "total_tasks": len(filtered_tasks),
                        "email_tasks": sources.get('email', 0),
                        "meeting_tasks": sources.get('meeting', 0),
                        "log_tasks": sources.get('log', 0)
                    },
                    "filters_applied": {
                        # Phase 1
                        "status": status,
                        "priority": priority,
                        "assignee": assignee,
                        # Phase 2
                        "deadline_before": deadline_before,
                        "deadline_after": deadline_after,
                        "created_after": created_after,
                        "created_before": created_before,
                        "source": source,
                        "department": department,
                        # Phase 3
                        "search": search,
                        "search_in": search_in,
                        # Phase 4
                        "page": page,
                        "limit": limit,
                        "sort_by": sort_by,
                        "order": order
                    },
                    "filter_statistics": filters_stats,
                    "format": "unified",
                    "system": "unified"
                }
        
        # LEGACY: Utiliser l'ancien syst√®me si unifi√© non disponible
        else:
            with open(DATA_FILE, "r", encoding="utf-8") as f:
                tasks = json.load(f)
            
            # üß† FILTRAGE INTELLIGENT PHASE 1: Appliquer filtres sur syst√®me legacy aussi
            filtered_legacy_tasks = tasks
            total_before_filter = len(tasks)
            
            if status:
                filtered_legacy_tasks = [t for t in filtered_legacy_tasks if smart_filter_match(t.get('statut', ''), status, 'status')]
            if priority:
                filtered_legacy_tasks = [t for t in filtered_legacy_tasks if smart_filter_match(t.get('priorite', ''), priority, 'priority')]
            if assignee:
                filtered_legacy_tasks = [t for t in filtered_legacy_tasks if assignee.lower() in t.get('responsable', '').lower()]
            
            # üöÄ FILTRAGE AVANC√â PHASE 2: Appliquer aussi sur legacy
            filtered_legacy_tasks = filter_by_date_range(
                filtered_legacy_tasks, 
                deadline_before=deadline_before,
                deadline_after=deadline_after,
                created_after=created_after,
                created_before=created_before
            )
            
            if source:
                filtered_legacy_tasks = filter_by_source(filtered_legacy_tasks, source)
            
            if department:
                filtered_legacy_tasks = filter_by_department(filtered_legacy_tasks, department)
            
            # üîç FILTRAGE RECHERCHE PHASE 3: Appliquer recherche textuelle pour system legacy
            if search:
                # Parser les champs de recherche
                search_fields = []
                if search_in == "all":
                    search_fields = ["all"]
                elif search_in == "responsable":
                    search_fields = ["responsable"]
                else:  # d√©faut: description
                    search_fields = ["description"]
                
                filtered_legacy_tasks = filter_by_search(filtered_legacy_tasks, search, search_fields)
            
            # üìÑ PHASE 4: TRI ET PAGINATION pour syst√®me legacy complet
            # Appliquer le tri
            if sort_by == "relevance" and search:
                # Tri par pertinence de recherche
                filtered_legacy_tasks = sort_by_relevance(filtered_legacy_tasks, search)
            elif sort_by:
                # Tri standard
                filtered_legacy_tasks = sort_tasks(filtered_legacy_tasks, sort_by, order)
            
            # Appliquer la pagination
            pagination_result = paginate_tasks(filtered_legacy_tasks, page, limit)
            paginated_tasks = pagination_result["tasks"]
            pagination_info = pagination_result["pagination"]
            
            return {
                "tasks": paginated_tasks,  # T√¢ches de la page courante
                "total": len(filtered_legacy_tasks),  # Total apr√®s filtres mais avant pagination
                "total_before_filter": total_before_filter,
                "pagination": pagination_info,
                "sorting": {
                    "sort_by": sort_by,
                    "order": order,
                    "available_sorts": get_available_sort_fields()
                },
                "filters_applied": {
                    # Phase 1
                    "status": status,
                    "priority": priority,
                    "assignee": assignee,
                    # Phase 2
                    "deadline_before": deadline_before,
                    "deadline_after": deadline_after,
                    "created_after": created_after,
                    "created_before": created_before,
                    "source": source,
                    "department": department,
                    # Phase 3
                    "search": search,
                    "search_in": search_in,
                    # Phase 4
                    "page": page,
                    "limit": limit,
                    "sort_by": sort_by,
                    "order": order
                },
                "format": "legacy",
                "system": "legacy"
            }
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur lecture t√¢ches: {str(e)}")

@app.post("/watcher/start")
def start_watcher():
    """D√©marrer la surveillance"""
    try:
        service = get_background_service()
        if service.start_service():
            return {
                "status": "success",
                "message": "Service de surveillance d√©marr√©",
                "timestamp": datetime.now().isoformat()
            }
        else:
            raise HTTPException(status_code=500, detail="Impossible de d√©marrer la surveillance")
    except Exception as e:
        return {
            "status": "error",
            "message": f"Service non disponible: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }

@app.post("/watcher/stop")
def stop_watcher():
    """Arr√™ter la surveillance"""
    try:
        service = get_background_service()
        if service.stop_service():
            return {
                "status": "success",
                "message": "Service de surveillance arr√™t√©", 
                "timestamp": datetime.now().isoformat()
            }
        else:
            raise HTTPException(status_code=500, detail="Impossible d'arr√™ter la surveillance")
    except Exception as e:
        return {
            "status": "error",
            "message": f"Service non disponible: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }

@app.get("/watcher/status")
def get_watcher_status():
    """√âtat de la surveillance"""
    try:
        service = get_background_service()
        return service.get_service_status()
    except Exception as e:
        return {
            "status": "error",
            "message": f"Service non disponible: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }

# =====================================
# üéØ NOUVEAUX ENDPOINTS MEETINGS
# =====================================

@app.get("/traiter-meetings")
def api_traiter_meetings_unifie(
    use_rate_limiting: bool = False,
    use_batch_processing: bool = True,
    use_cache: bool = True,
    use_optimized_prompts: bool = True
    ):
    """üéØ ENDPOINT UNIFI√â: Traitement r√©unions avec toutes les am√©liorations"""
    try:
        resultat = pipeline_traiter_reunions(
            use_rate_limiting=use_rate_limiting,
            use_batch_processing=use_batch_processing,
            use_cache=use_cache,
            use_optimized_prompts=use_optimized_prompts
        )
        
        optimisations_actives = []
        if use_cache:
            optimisations_actives.append("Cache anti-doublon")
        if use_batch_processing:
            optimisations_actives.append("Batch processing intelligent")
        if use_rate_limiting:
            optimisations_actives.append("Rate limiting + Queue")
        if use_optimized_prompts:
            optimisations_actives.append("Prompts IA optimis√©s")
        
        return {
            "status": "success",
            "mode": "production_optimized_processing",
            "optimisations_actives": optimisations_actives,
            "details": resultat,
            "message": f"R√©unions trait√©es avec {len(optimisations_actives)} optimisations actives"
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur traitement unifi√© r√©unions: {str(e)}")

@app.post("/meetings/process")
async def process_meeting_manual(meeting: MeetingInput):
    """Traite une r√©union sp√©cifique manuellement"""
    try:
        processor = get_meeting_processor()
        
        # Convertir en format dict
        meeting_dict = {
            "id": f"meeting_manual_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "titre": meeting.titre,
            "date_reunion": meeting.date_reunion,
            "heure_debut": meeting.heure_debut,
            "heure_fin": meeting.heure_fin,
            "duree_minutes": meeting.duree_minutes,
            "lieu": meeting.lieu,
            "organisateur": meeting.organisateur.dict(),
            "participants": [p.dict() for p in meeting.participants],
            "ordre_du_jour": meeting.ordre_du_jour,
            "transcription": meeting.transcription,
            "departement": meeting.departement,
            "projet_associe": meeting.projet_associe,
            "priorite_meeting": meeting.priorite_meeting,
            "type_reunion": meeting.type_reunion,
            "statut_traitement": "non_trait√©",
            "date_ajout": datetime.now().isoformat(),
            "tags": meeting.tags,
            "fichiers_associes": [f.dict() for f in meeting.fichiers_associes],
            "decisions_prises": [],
            "actions_identifiees": [],
            "nb_taches_extraites": 0,
            "hash_transcription": "",
            "resume_reunion": "",
            "points_importants": [],
            "prochaines_etapes": []
        }
        
        # Traiter la r√©union
        result = processor.process_meeting(meeting_dict)
        
        return {
            "status": "success",
            "meeting_processed": result,
            "message": f"R√©union '{meeting.titre}' trait√©e avec succ√®s"
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur traitement r√©union: {str(e)}")

@app.post("/meetings/transcription-simple")
async def process_transcription_simple(data: TranscriptionSimple):
    """Traite une transcription simple sans m√©tadonn√©es compl√®tes"""
    try:
        processor = get_meeting_processor()
        
        # Cr√©er r√©union minimale
        meeting_dict = {
            "id": f"meeting_transcription_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "titre": data.meeting_title,
            "date_reunion": data.meeting_date,
            "heure_debut": "00:00",
            "heure_fin": "00:00", 
            "duree_minutes": 0,
            "lieu": "Non sp√©cifi√©",
            "organisateur": {
                "nom": "Non sp√©cifi√©",
                "email": "organizer@unknown.com",
                "role": "Organisateur"
            },
            "participants": [
                {
                    "nom": nom,
                    "email": f"{nom.lower().replace(' ', '.')}@unknown.com",
                    "role": "Participant",
                    "present": True
                } for nom in data.participants
            ] if data.participants else [
                {
                    "nom": "Participant inconnu", 
                    "email": "participant@unknown.com",
                    "role": "Participant",
                    "present": True
                }
            ],
            "ordre_du_jour": ["Points discut√©s pendant la r√©union"],
            "transcription": data.transcription,
            "departement": "Non sp√©cifi√©",
            "projet_associe": "",
            "priorite_meeting": "normale",
            "type_reunion": "general",
            "statut_traitement": "non_trait√©",
            "date_ajout": datetime.now().isoformat(),
            "tags": [],
            "fichiers_associes": [],
            "decisions_prises": [],
            "actions_identifiees": [],
            "nb_taches_extraites": 0,
            "hash_transcription": "",
            "resume_reunion": "",
            "points_importants": [],
            "prochaines_etapes": []
        }
        
        # Traiter la r√©union
        result = processor.process_meeting(meeting_dict)
        
        return {
            "status": "success",
            "transcription_processed": result,
            "message": f"Transcription '{data.meeting_title}' trait√©e avec succ√®s"
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur traitement transcription: {str(e)}")

@app.get("/tasks/stats")
def get_tasks_statistics():
    """
    üéØ R√©cup√©rer les statistiques globales des t√¢ches (KPI corrects)
    
    Retourne les vraies m√©triques bas√©es sur TOUTES les t√¢ches de la base
    """
    try:
        # üöÄ UTILISER LE SYST√àME UNIFI√â: M√™me source que /all-tasks
        if UNIFIED_SYSTEM_AVAILABLE and os.path.exists(UNIFIED_TASKS_FILE):
            unified_manager = get_unified_task_manager()
            all_tasks = unified_manager.load_all_tasks()
        else:
            # Fallback: utiliser l'ancien syst√®me
            with open(DATA_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
            all_tasks = data if isinstance(data, list) else []
        
        # Calculer les statistiques
        total_tasks = len(all_tasks)
        
        if total_tasks == 0:
            return {
                "total": 0,
                "by_status": {"completed": 0, "in_progress": 0, "pending": 0, "rejected": 0},
                "by_priority": {"urgent": 0, "high": 0, "medium": 0, "low": 0},
                "completion_rate": 0.0
            }
        
        completed_tasks = len([t for t in all_tasks if t.get('statut') == 'completed'])
        in_progress_tasks = len([t for t in all_tasks if t.get('statut') == 'in_progress'])
        pending_tasks = len([t for t in all_tasks if t.get('statut') == 'pending'])
        rejected_tasks = len([t for t in all_tasks if t.get('statut') == 'rejected'])
        
        # Statistiques par priorit√©
        high_priority_tasks = len([t for t in all_tasks if t.get('priorite') == 'high'])
        urgent_priority_tasks = len([t for t in all_tasks if t.get('priorite') == 'urgent'])
        medium_priority_tasks = len([t for t in all_tasks if t.get('priorite') == 'medium'])
        low_priority_tasks = len([t for t in all_tasks if t.get('priorite') == 'low'])
        
        return {
            "total": total_tasks,
            "by_status": {
                "completed": completed_tasks,
                "in_progress": in_progress_tasks,
                "pending": pending_tasks,
                "rejected": rejected_tasks
            },
            "by_priority": {
                "urgent": urgent_priority_tasks,
                "high": high_priority_tasks,
                "medium": medium_priority_tasks,
                "low": low_priority_tasks
            },
            "completion_rate": round((completed_tasks / total_tasks * 100) if total_tasks > 0 else 0, 1)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur r√©cup√©ration statistiques: {str(e)}")

@app.get("/meetings")
async def list_meetings(
    departement: str = None,
    type_reunion: str = None,
    statut: str = None
):
    """Liste les r√©unions avec filtres optionnels"""
    try:
        processor = get_meeting_processor()
        meetings = processor.load_meetings()
        
        # Appliquer filtres
        if departement:
            meetings = [m for m in meetings if m.get("departement", "").lower() == departement.lower()]
        if type_reunion:
            meetings = [m for m in meetings if m.get("type_reunion", "").lower() == type_reunion.lower()]
        if statut:
            meetings = [m for m in meetings if m.get("statut_traitement", "").lower() == statut.lower()]
        
        return {
            "meetings": meetings,
            "total": len(meetings),
            "filters_applied": {
                "departement": departement,
                "type_reunion": type_reunion,
                "statut": statut
            }
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur r√©cup√©ration r√©unions: {str(e)}")

@app.get("/meetings/{meeting_id}")
async def get_meeting_details(meeting_id: str):
    """D√©tails complets d'une r√©union"""
    try:
        processor = get_meeting_processor()
        meeting = processor.get_meeting_by_id(meeting_id)
        
        if not meeting:
            raise HTTPException(status_code=404, detail=f"R√©union {meeting_id} non trouv√©e")
        
        return {
            "meeting": meeting,
            "status": "found"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur r√©cup√©ration r√©union: {str(e)}")

@app.get("/meetings/{meeting_id}/tasks")
async def get_meeting_tasks(meeting_id: str):
    """T√¢ches extraites d'une r√©union sp√©cifique"""
    try:
        processor = get_meeting_processor()
        tasks = processor.get_tasks_by_meeting(meeting_id)
        
        return {
            "meeting_id": meeting_id,
            "tasks": tasks,
            "total_tasks": len(tasks)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur r√©cup√©ration t√¢ches r√©union: {str(e)}")

@app.get("/meetings/stats/global")
async def get_meetings_statistics():
    """Statistiques globales des r√©unions"""
    try:
        processor = get_meeting_processor()
        meetings = processor.load_meetings()
        meeting_tasks = processor.load_meeting_tasks()
        
        stats = {
            "total_meetings": len(meetings),
            "meetings_processed": len([m for m in meetings if m.get("statut_traitement") == "trait√©"]),
            "meetings_pending": len([m for m in meetings if m.get("statut_traitement") == "non_trait√©"]),
            "total_tasks_extracted": len(meeting_tasks),
            "departments": {},
            "meeting_types": {},
            "average_tasks_per_meeting": 0
        }
        
        # Statistiques par d√©partement
        for meeting in meetings:
            dept = meeting.get("departement", "Non sp√©cifi√©")
            stats["departments"][dept] = stats["departments"].get(dept, 0) + 1
        
        # Statistiques par type de r√©union
        for meeting in meetings:
            meeting_type = meeting.get("type_reunion", "Non sp√©cifi√©")
            stats["meeting_types"][meeting_type] = stats["meeting_types"].get(meeting_type, 0) + 1
        
        # Moyenne t√¢ches par r√©union
        if stats["meetings_processed"] > 0:
            stats["average_tasks_per_meeting"] = round(stats["total_tasks_extracted"] / stats["meetings_processed"], 2)
        
        return stats
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur statistiques r√©unions: {str(e)}")

# =====================================
# ÔøΩÔ∏è NOUVEAUX ENDPOINTS - PHASE 7
# SYST√àME DE TAGS COMPLET
# =====================================

@app.post("/tasks/create")
async def create_task_with_tags(task_data: TaskCreate):
    """
    ‚ú® Cr√©er une nouvelle t√¢che avec support complet des tags
    
    Fonctionnalit√©s :
    - Validation compl√®te des donn√©es
    - Support des tags avec normalisation automatique
    - G√©n√©ration d'ID unique
    - Horodatage automatique
    """
    try:
        if not UNIFIED_SYSTEM_AVAILABLE:
            raise HTTPException(status_code=503, detail="Syst√®me unifi√© non disponible")
        
        # G√©n√©ration ID unique
        task_id = str(uuid.uuid4())
        timestamp = datetime.now().isoformat()
        
        # Pr√©parer la t√¢che pour le gestionnaire unifi√©
        task_data_for_manager = {
            "description": task_data.title + " - " + task_data.description,
            "responsable": "user",
            "deadline": task_data.deadline,
            "priorite": task_data.priority,
            "statut": "pending",
            "source": "manual_api",
            "type": "explicite",
            "tags": getattr(task_data, 'tags', []),
            "source_metadata": {
                "title": task_data.title,
                "description": task_data.description,
                "department": task_data.department,
                "created_via": "api_endpoint"
            }
        }
        
        # Sauvegarder via le gestionnaire unifi√©
        unified_manager = get_unified_task_manager()
        task_id = unified_manager.add_task(task_data_for_manager)
        
        # R√©cup√©rer la t√¢che cr√©√©e pour la retourner
        created_task = unified_manager.get_task_by_id(task_id)
        
        return {
            "message": "T√¢che cr√©√©e avec succ√®s",
            "task_id": task_id,
            "tags_count": len(getattr(task_data, 'tags', [])),
            "created_at": created_task.get('created_at'),
            "task": created_task
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur cr√©ation t√¢che: {str(e)}")

@app.get("/tasks/tags")
async def get_all_tags():
    """
    üè∑Ô∏è R√©cup√©rer tous les tags utilis√©s dans le syst√®me
    
    Retourne :
    - Liste de tous les tags uniques
    - Nombre d'utilisation de chaque tag
    - Tags tri√©s par popularit√©
    """
    try:
        if not UNIFIED_SYSTEM_AVAILABLE:
            raise HTTPException(status_code=503, detail="Syst√®me unifi√© non disponible")
        
        # R√©cup√©rer toutes les t√¢ches
        unified_manager = get_unified_task_manager()
        all_tasks = unified_manager.load_all_tasks()
        
        # Compter les tags
        tag_counts = {}
        for task in all_tasks:
            task_tags = task.get('tags', [])
            for tag in task_tags:
                tag_counts[tag] = tag_counts.get(tag, 0) + 1
        
        # Trier par popularit√©
        sorted_tags = sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)
        
        return {
            "message": "Tags r√©cup√©r√©s avec succ√®s",
            "total_unique_tags": len(tag_counts),
            "total_tasks_with_tags": len([t for t in all_tasks if t.get('tags')]),
            "tags": {
                "by_popularity": sorted_tags,
                "alphabetical": sorted(tag_counts.keys()),
                "counts": tag_counts
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur r√©cup√©ration tags: {str(e)}")

@app.get("/tasks/tags/{tag_name}/tasks")
async def get_tasks_by_tag(tag_name: str, limit: int = 20, page: int = 1):
    """
    üîç R√©cup√©rer toutes les t√¢ches avec un tag sp√©cifique
    
    Param√®tres :
    - tag_name: Nom du tag √† rechercher
    - limit: Nombre de t√¢ches par page (d√©faut: 20)
    - page: Num√©ro de page (d√©faut: 1)
    """
    try:
        if not UNIFIED_SYSTEM_AVAILABLE:
            raise HTTPException(status_code=503, detail="Syst√®me unifi√© non disponible")
        
        # Normaliser le tag recherch√©
        normalized_tag = tag_name.strip().lower().replace(' ', '-').replace('_', '-')
        
        # R√©cup√©rer toutes les t√¢ches avec ce tag
        unified_manager = get_unified_task_manager()
        all_tasks = unified_manager.load_all_tasks()
        
        # Filtrer par tag
        tagged_tasks = [task for task in all_tasks if normalized_tag in task.get('tags', [])]
        
        # Pagination
        start_idx = (page - 1) * limit
        end_idx = start_idx + limit
        page_tasks = tagged_tasks[start_idx:end_idx]
        
        total_pages = (len(tagged_tasks) + limit - 1) // limit
        
        pagination_result = {
            "tasks": page_tasks,
            "pagination": {
                "current_page": page,
                "per_page": limit,
                "total_pages": total_pages,
                "total_items": len(tagged_tasks),
                "has_next": page < total_pages,
                "has_prev": page > 1
            }
        }
        
        return {
            "message": f"T√¢ches avec tag '{tag_name}' r√©cup√©r√©es",
            "tag": normalized_tag,
            "total_tasks": len(tagged_tasks),
            "tasks": pagination_result["tasks"],
            "pagination": pagination_result["pagination"]
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur r√©cup√©ration t√¢ches par tag: {str(e)}")

@app.get("/tasks/tags/popular")
async def get_popular_tags(limit: int = 10):
    """
    üìä R√©cup√©rer les tags les plus populaires
    
    - Tri√©s par nombre d'utilisation d√©croissant
    - Limite configurable (d√©faut: 10)
    """
    try:
        if not UNIFIED_SYSTEM_AVAILABLE:
            raise HTTPException(status_code=503, detail="Syst√®me unifi√© non disponible")
        
        # R√©cup√©rer tous les tags
        response = await get_all_tags()
        all_tags_data = response["tags"]["by_popularity"]
        
        # Limiter aux plus populaires
        popular_tags = all_tags_data[:limit]
        
        return {
            "message": f"Top {limit} tags populaires",
            "popular_tags": popular_tags,
            "total_tags_available": len(all_tags_data)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur tags populaires: {str(e)}")

@app.get("/tasks/tags/suggestions")
async def get_tag_suggestions(description: str = None, task_id: str = None):
    """
    ü§ñ Obtenir des suggestions de tags intelligentes
    
    Bas√© sur :
    - Description de la t√¢che (si fournie)
    - Tags existants dans le syst√®me
    - Mots-cl√©s fr√©quents
    """
    try:
        if not UNIFIED_SYSTEM_AVAILABLE:
            raise HTTPException(status_code=503, detail="Syst√®me unifi√© non disponible")
        
        # R√©cup√©rer tags existants pour suggestions
        response = await get_all_tags()
        existing_tags = list(response["tags"]["counts"].keys())
        
        suggestions = []
        
        # Si description fournie, analyser pour sugg√©rer
        if description:
            description_lower = description.lower()
            
            # Mots-cl√©s ‚Üí tags sugg√©r√©s
            keyword_mapping = {
                "urgent": ["urgent", "prioritaire", "asap"],
                "bug": ["bug", "erreur", "probl√®me"],
                "feature": ["feature", "fonctionnalit√©", "nouveau"],
                "r√©union": ["r√©union", "meeting", "rendez-vous"],
                "rapport": ["rapport", "document", "analysis"],
                "client": ["client", "customer", "externe"],
                "interne": ["interne", "√©quipe", "team"],
                "s√©curit√©": ["s√©curit√©", "security", "protection"],
                "performance": ["performance", "optimisation", "vitesse"],
                "test": ["test", "testing", "qa"]
            }
            
            for keyword, tags in keyword_mapping.items():
                if any(word in description_lower for word in tags):
                    suggestions.extend(tags)
            
            # Ajouter suggestions bas√©es sur tags existants similaires
            for existing_tag in existing_tags:
                if any(word in existing_tag for word in description_lower.split()):
                    suggestions.append(existing_tag)
        
        # Si task_id fourni, analyser la t√¢che
        if task_id:
            unified_manager = get_unified_task_manager()
            task = unified_manager.get_task_by_id(task_id)
            if task:
                task_desc = task.get('description', '')
                # R√©cursif pour analyser description de la t√¢che
                desc_suggestions = await get_tag_suggestions(description=task_desc)
                suggestions.extend(desc_suggestions.get('suggested_tags', []))
        
        # √âliminer doublons et limiter
        unique_suggestions = list(set(suggestions))[:10]
        
        # Ajouter quelques tags populaires
        popular_response = await get_popular_tags(limit=5)
        popular_tags = [tag[0] for tag in popular_response["popular_tags"]]
        
        return {
            "message": "Suggestions de tags g√©n√©r√©es",
            "suggested_tags": unique_suggestions,
            "popular_tags": popular_tags,
            "all_existing_tags": existing_tags[:20]  # Premiers 20 pour r√©f√©rence
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur suggestions tags: {str(e)}")

@app.post("/tasks/{task_id}/tags")
async def add_tags_to_task(task_id: str, tag_data: TaskTags):
    """
    üè∑Ô∏è Ajouter des tags √† une t√¢che
    
    - Validation automatique des tags (format, longueur)
    - Normalisation automatique (lowercase, tirets)
    - √âvite les doublons
    - Limite √† 7 tags maximum par t√¢che
    """
    try:
        if not UNIFIED_SYSTEM_AVAILABLE:
            raise HTTPException(status_code=503, detail="Syst√®me unifi√© non disponible")
        
        # R√©cup√©rer la t√¢che
        unified_manager = get_unified_task_manager()
        existing_task = unified_manager.get_task_by_id(task_id)
        
        if not existing_task:
            raise HTTPException(status_code=404, detail=f"T√¢che '{task_id}' non trouv√©e")
        
        # Tags actuels
        current_tags = existing_task.get('tags', [])
        new_tags = tag_data.tags
        
        # Combiner et √©liminer doublons
        combined_tags = list(set(current_tags + new_tags))
        
        # V√©rifier limite maximum
        if len(combined_tags) > 7:
            raise HTTPException(
                status_code=400, 
                detail=f"Limite de 7 tags d√©pass√©e. Actuel: {len(current_tags)}, √† ajouter: {len(new_tags)}, total: {len(combined_tags)}"
            )
        
        # Pr√©parer mise √† jour
        updates = {
            "tags": combined_tags,
            "updated_at": datetime.now().isoformat()
        }
        
        # Historique
        added_tags = [tag for tag in new_tags if tag not in current_tags]
        history_entry = {
            "action": "tags_added",
            "timestamp": datetime.now().isoformat(),
            "user": "user",
            "details": f"Tags ajout√©s: {', '.join(added_tags)}" if added_tags else "Aucun nouveau tag"
        }
        
        # Mettre √† jour
        updated_task = unified_manager.update_task(task_id, updates, history_entry)
        
        return {
            "message": "Tags ajout√©s avec succ√®s",
            "task_id": task_id,
            "tags_added": added_tags,
            "tags_already_present": [tag for tag in new_tags if tag in current_tags],
            "total_tags": len(combined_tags),
            "all_tags": combined_tags,
            "updated_at": updates["updated_at"],
            "task": updated_task
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur ajout tags: {str(e)}")

@app.delete("/tasks/{task_id}/tags/{tag_name}")
async def remove_tag_from_task(task_id: str, tag_name: str):
    """
    üóëÔ∏è Retirer un tag sp√©cifique d'une t√¢che
    
    - Supprime uniquement le tag sp√©cifi√©
    - Garde les autres tags intacts
    - Erreur si tag non pr√©sent
    """
    try:
        if not UNIFIED_SYSTEM_AVAILABLE:
            raise HTTPException(status_code=503, detail="Syst√®me unifi√© non disponible")
        
        # Normaliser le tag
        normalized_tag = tag_name.strip().lower().replace(' ', '-').replace('_', '-')
        
        # R√©cup√©rer la t√¢che
        unified_manager = get_unified_task_manager()
        existing_task = unified_manager.get_task_by_id(task_id)
        
        if not existing_task:
            raise HTTPException(status_code=404, detail=f"T√¢che '{task_id}' non trouv√©e")
        
        # Tags actuels
        current_tags = existing_task.get('tags', [])
        
        # V√©rifier si tag pr√©sent
        if normalized_tag not in current_tags:
            raise HTTPException(status_code=404, detail=f"Tag '{tag_name}' non trouv√© dans cette t√¢che")
        
        # Retirer le tag
        updated_tags = [tag for tag in current_tags if tag != normalized_tag]
        
        # Pr√©parer mise √† jour
        updates = {
            "tags": updated_tags,
            "updated_at": datetime.now().isoformat()
        }
        
        # Historique
        history_entry = {
            "action": "tag_removed",
            "timestamp": datetime.now().isoformat(),
            "user": "user",
            "details": f"Tag retir√©: {normalized_tag}"
        }
        
        # Mettre √† jour
        updated_task = unified_manager.update_task(task_id, updates, history_entry)
        
        return {
            "message": "Tag retir√© avec succ√®s",
            "task_id": task_id,
            "tag_removed": normalized_tag,
            "remaining_tags": updated_tags,
            "total_tags": len(updated_tags),
            "updated_at": updates["updated_at"],
            "task": updated_task
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur suppression tag: {str(e)}")

# =====================================
# ÔøΩüÜï NOUVEAUX ENDPOINTS - PHASE 5
# GESTION INDIVIDUELLE DES T√ÇCHES
# =====================================

@app.get("/tasks/{task_id}")
async def get_task_detail(task_id: str):
    """
    üîç R√©cup√©rer le d√©tail complet d'une t√¢che par son ID
    
    Retourne toutes les informations d'une t√¢che :
    - D√©tails de base (description, priorit√©, statut, etc.)
    - Informations de validation humaine
    - Historique des actions
    - M√©tadonn√©es sources (email/meeting)
    - √Çge de la t√¢che en jours
    """
    try:
        if not UNIFIED_SYSTEM_AVAILABLE:
            raise HTTPException(status_code=503, detail="Syst√®me unifi√© non disponible")
        
        # R√©cup√©rer la t√¢che depuis le syst√®me unifi√©
        unified_manager = get_unified_task_manager()
        task = unified_manager.get_task_by_id(task_id)
        
        if not task:
            raise HTTPException(status_code=404, detail=f"T√¢che '{task_id}' non trouv√©e")
        
        # Enrichir avec informations suppl√©mentaires
        enriched_task = {
            **task,
            "validated": task.get("validated", False),
            "rejected": task.get("rejected", False),
            "completed": task.get("completed", False),
            "history_count": len(task.get("history", [])),
            "last_modified": task.get("updated_at"),
            "age_days": unified_manager.calculate_task_age(task.get("created_at")),
            "progress": task.get("progress", 0),
            "completion_notes": task.get("completion_notes"),
            "rejection_reason": task.get("rejection_reason")
        }
        
        return {
            "message": "D√©tail t√¢che r√©cup√©r√© avec succ√®s",
            "task": enriched_task
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur r√©cup√©ration t√¢che: {str(e)}")

@app.patch("/tasks/{task_id}/validate")
async def validate_task(task_id: str):
    """
    ‚úÖ Valider une t√¢che (marquer comme v√©rifi√©e par un humain)
    
    Actions effectu√©es :
    - Marquer validated = true
    - Ajouter timestamp de validation
    - Cr√©er entr√©e dans l'historique
    - Mettre √† jour updated_at
    """
    try:
        if not UNIFIED_SYSTEM_AVAILABLE:
            raise HTTPException(status_code=503, detail="Syst√®me unifi√© non disponible")
        
        # R√©cup√©rer la t√¢che
        unified_manager = get_unified_task_manager()
        task = unified_manager.get_task_by_id(task_id)
        
        if not task:
            raise HTTPException(status_code=404, detail=f"T√¢che '{task_id}' non trouv√©e")
        
        # V√©rifier si d√©j√† valid√©e
        if task.get("validated", False):
            raise HTTPException(status_code=400, detail="T√¢che d√©j√† valid√©e")
        
        # Pr√©parer les mises √† jour
        updates = {
            "validated": True,
            "validation_status": "validated",
            "validated_at": datetime.now().isoformat(),
            "validated_by": "user",  # TODO: r√©cup√©rer de l'authentification
            "updated_at": datetime.now().isoformat()
        }
        
        # Cr√©er entr√©e d'historique
        history_entry = {
            "action": "validated",
            "timestamp": datetime.now().isoformat(),
            "user": "user",
            "details": "T√¢che valid√©e manuellement par utilisateur"
        }
        
        # Mettre √† jour la t√¢che
        updated_task = unified_manager.update_task(task_id, updates, history_entry)
        
        if not updated_task:
            raise HTTPException(status_code=500, detail="Erreur lors de la validation")
        
        return {
            "message": "T√¢che valid√©e avec succ√®s",
            "task_id": task_id,
            "validated_at": updates["validated_at"],
            "task": updated_task
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur validation t√¢che: {str(e)}")

@app.patch("/tasks/{task_id}/reject")
async def reject_task(task_id: str, rejection_reason: str = "Non sp√©cifi√©e"):
    """
    ‚ùå Rejeter une t√¢che avec raison optionnelle
    
    Actions effectu√©es :
    - Marquer statut = rejected
    - Marquer rejected = true
    - Enregistrer raison du rejet
    - Ajouter timestamp de rejet
    - Cr√©er entr√©e dans l'historique
    """
    try:
        if not UNIFIED_SYSTEM_AVAILABLE:
            raise HTTPException(status_code=503, detail="Syst√®me unifi√© non disponible")
        
        # R√©cup√©rer la t√¢che
        unified_manager = get_unified_task_manager()
        task = unified_manager.get_task_by_id(task_id)
        
        if not task:
            raise HTTPException(status_code=404, detail=f"T√¢che '{task_id}' non trouv√©e")
        
        # V√©rifier si d√©j√† rejet√©e
        if task.get("rejected", False):
            raise HTTPException(status_code=400, detail="T√¢che d√©j√† rejet√©e")
        
        # V√©rifier si d√©j√† termin√©e
        if task.get("statut") == "completed":
            raise HTTPException(status_code=400, detail="Impossible de rejeter une t√¢che termin√©e")
        
        # Pr√©parer les mises √† jour
        updates = {
            "statut": "rejected",
            "rejected": True,
            "validation_status": "rejected",
            "rejected_at": datetime.now().isoformat(),
            "rejected_by": "user",
            "rejection_reason": rejection_reason,
            "updated_at": datetime.now().isoformat()
        }
        
        # Cr√©er entr√©e d'historique
        history_entry = {
            "action": "rejected",
            "timestamp": datetime.now().isoformat(),
            "user": "user",
            "details": f"T√¢che rejet√©e. Raison: {rejection_reason}"
        }
        
        # Mettre √† jour la t√¢che
        updated_task = unified_manager.update_task(task_id, updates, history_entry)
        
        if not updated_task:
            raise HTTPException(status_code=500, detail="Erreur lors du rejet")
        
        return {
            "message": "T√¢che rejet√©e avec succ√®s",
            "task_id": task_id,
            "rejected_at": updates["rejected_at"],
            "reason": rejection_reason,
            "task": updated_task
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur rejet t√¢che: {str(e)}")

@app.patch("/tasks/{task_id}/complete")
async def complete_task(task_id: str, completion_notes: str = None):
    """
    ‚úÖ Marquer une t√¢che comme termin√©e
    
    Actions effectu√©es :
    - Marquer statut = completed
    - Marquer completed = true
    - Progression √† 100%
    - Ajouter notes de compl√©tion optionnelles
    - Ajouter timestamp de compl√©tion
    - Cr√©er entr√©e dans l'historique
    """
    try:
        if not UNIFIED_SYSTEM_AVAILABLE:
            raise HTTPException(status_code=503, detail="Syst√®me unifi√© non disponible")
        
        # R√©cup√©rer la t√¢che
        unified_manager = get_unified_task_manager()
        task = unified_manager.get_task_by_id(task_id)
        
        if not task:
            raise HTTPException(status_code=404, detail=f"T√¢che '{task_id}' non trouv√©e")
        
        # V√©rifier si d√©j√† termin√©e
        if task.get("statut") == "completed" or task.get("completed", False):
            raise HTTPException(status_code=400, detail="T√¢che d√©j√† termin√©e")
        
        # V√©rifier si rejet√©e
        if task.get("rejected", False):
            raise HTTPException(status_code=400, detail="Impossible de terminer une t√¢che rejet√©e")
        
        # Pr√©parer les mises √† jour
        updates = {
            "statut": "completed",
            "completed": True,
            "validation_status": "completed",
            "completed_at": datetime.now().isoformat(),
            "completed_by": "user",
            "completion_notes": completion_notes,
            "progress": 100,
            "updated_at": datetime.now().isoformat()
        }
        
        # Cr√©er entr√©e d'historique
        details = f"T√¢che termin√©e avec succ√®s"
        if completion_notes:
            details += f". Notes: {completion_notes}"
        
        history_entry = {
            "action": "completed",
            "timestamp": datetime.now().isoformat(),
            "user": "user",
            "details": details
        }
        
        # Mettre √† jour la t√¢che
        updated_task = unified_manager.update_task(task_id, updates, history_entry)
        
        if not updated_task:
            raise HTTPException(status_code=500, detail="Erreur lors de la compl√©tion")
        
        return {
            "message": "T√¢che termin√©e avec succ√®s",
            "task_id": task_id,
            "completed_at": updates["completed_at"],
            "notes": completion_notes,
            "progress": 100,
            "task": updated_task
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur compl√©tion t√¢che: {str(e)}")

# =====================================
# üÜï NOUVEAUX ENDPOINTS - PHASE 6
# MODIFICATION ET √âDITION DES T√ÇCHES
# =====================================

@app.put("/tasks/{task_id}")
async def update_task_complete(task_id: str, task_data: TaskUpdateComplete):
    """
    üìù Modifier compl√®tement une t√¢che (PUT)
    
    Remplace tous les champs modifiables de la t√¢che.
    Pr√©serve automatiquement : id, created_at, source, source_metadata, history
    
    Champs modifiables :
    - description (obligatoire, 3-500 caract√®res)
    - responsable (obligatoire, 1-100 caract√®res)  
    - priorite (obligatoire: high, medium, low)
    - statut (obligatoire: pending, in_progress, completed, cancelled, rejected)
    - deadline (optionnel: YYYY-MM-DD ou null)
    - department (optionnel, max 50 caract√®res)
    """
    try:
        if not UNIFIED_SYSTEM_AVAILABLE:
            raise HTTPException(status_code=503, detail="Syst√®me unifi√© non disponible")
        
        # R√©cup√©rer la t√¢che existante
        unified_manager = get_unified_task_manager()
        existing_task = unified_manager.get_task_by_id(task_id)
        
        if not existing_task:
            raise HTTPException(status_code=404, detail=f"T√¢che '{task_id}' non trouv√©e")
        
        # V√©rifier si la t√¢che peut √™tre modifi√©e
        if existing_task.get("rejected", False):
            raise HTTPException(status_code=400, detail="Impossible de modifier une t√¢che rejet√©e")
        
        # Pr√©parer les mises √† jour (tous les champs)
        updates = {
            "description": task_data.description,
            "responsable": task_data.responsable,
            "priorite": task_data.priorite,
            "statut": task_data.statut,
            "deadline": task_data.deadline,
            "department": task_data.department,
            "updated_at": datetime.now().isoformat()
        }
        
        # Cr√©er un r√©sum√© des changements pour l'historique
        changes = []
        for key, new_value in updates.items():
            if key == "updated_at":
                continue
            old_value = existing_task.get(key)
            if old_value != new_value:
                changes.append(f"{key}: '{old_value}' ‚Üí '{new_value}'")
        
        # Cr√©er entr√©e d'historique
        history_entry = {
            "action": "updated_complete",
            "timestamp": datetime.now().isoformat(),
            "user": "user",
            "details": f"Modification compl√®te. Changements: {', '.join(changes) if changes else 'Aucun changement'}"
        }
        
        # Mettre √† jour la t√¢che
        updated_task = unified_manager.update_task(task_id, updates, history_entry)
        
        if not updated_task:
            raise HTTPException(status_code=500, detail="Erreur lors de la modification")
        
        return {
            "message": "T√¢che modifi√©e avec succ√®s",
            "task_id": task_id,
            "changes_count": len(changes),
            "changes": changes,
            "updated_at": updates["updated_at"],
            "task": updated_task
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur modification t√¢che: {str(e)}")

@app.patch("/tasks/{task_id}/priority")
async def update_task_priority(task_id: str, priority_data: TaskPriorityUpdate):
    """
    üéØ Modifier uniquement la priorit√© d'une t√¢che
    
    Valeurs accept√©es : high, medium, low
    """
    try:
        if not UNIFIED_SYSTEM_AVAILABLE:
            raise HTTPException(status_code=503, detail="Syst√®me unifi√© non disponible")
        
        priority = priority_data.priority
        
        # R√©cup√©rer la t√¢che
        unified_manager = get_unified_task_manager()
        existing_task = unified_manager.get_task_by_id(task_id)
        
        if not existing_task:
            raise HTTPException(status_code=404, detail=f"T√¢che '{task_id}' non trouv√©e")
        
        # V√©rifier si changement n√©cessaire
        old_priority = existing_task.get("priorite", "medium")
        if old_priority == priority:
            return {
                "message": "Priorit√© inchang√©e",
                "task_id": task_id,
                "priority": priority,
                "task": existing_task
            }
        
        # Pr√©parer mise √† jour
        updates = {
            "priorite": priority,
            "updated_at": datetime.now().isoformat()
        }
        
        # Historique sp√©cifique
        history_entry = {
            "action": "priority_changed",
            "timestamp": datetime.now().isoformat(),
            "user": "user",
            "details": f"Priorit√© modifi√©e: {old_priority} ‚Üí {priority}"
        }
        
        # Mettre √† jour
        updated_task = unified_manager.update_task(task_id, updates, history_entry)
        
        return {
            "message": "Priorit√© modifi√©e avec succ√®s",
            "task_id": task_id,
            "old_priority": old_priority,
            "new_priority": priority,
            "updated_at": updates["updated_at"],
            "task": updated_task
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur modification priorit√©: {str(e)}")

@app.patch("/tasks/{task_id}/deadline")
async def update_task_deadline(task_id: str, deadline_data: TaskDeadlineUpdate):
    """
    üìÖ Modifier uniquement la deadline d'une t√¢che

    Format accept√© : YYYY-MM-DD ou null pour supprimer
    """
    try:
        if not UNIFIED_SYSTEM_AVAILABLE:
            raise HTTPException(status_code=503, detail="Syst√®me unifi√© non disponible")

        deadline = deadline_data.deadline

        # R√©cup√©rer la t√¢che
        unified_manager = get_unified_task_manager()
        existing_task = unified_manager.get_task_by_id(task_id)

        if not existing_task:
            raise HTTPException(status_code=404, detail=f"T√¢che '{task_id}' non trouv√©e")

        # V√©rifier si changement n√©cessaire
        old_deadline = existing_task.get("deadline")
        if old_deadline == deadline:
            return {
                "message": "Deadline inchang√©e",
                "task_id": task_id,
                "deadline": deadline,
                "task": existing_task
            }

        # Pr√©parer mise √† jour
        updates = {
            "deadline": deadline,
            "updated_at": datetime.now().isoformat()
        }

        # Historique sp√©cifique
        old_str = old_deadline or "non d√©finie"
        new_str = deadline or "supprim√©e"
        history_entry = {
            "action": "deadline_changed",
            "timestamp": datetime.now().isoformat(),
            "user": "user",
            "details": f"√âch√©ance modifi√©e: {old_str} ‚Üí {new_str}"
        }

        # Mettre √† jour
        updated_task = unified_manager.update_task(task_id, updates, history_entry)

        return {
            "message": "√âch√©ance modifi√©e avec succ√®s",
            "task_id": task_id,
            "old_deadline": old_deadline,
            "new_deadline": deadline,
            "updated_at": updates["updated_at"],
            "task": updated_task
        }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur modification √©ch√©ance: {str(e)}")

@app.patch("/tasks/{task_id}/description")
async def update_task_description(task_id: str, description_data: TaskDescriptionUpdate):
    """
    üìù Modifier uniquement la description d'une t√¢che
    
    Description : 3-500 caract√®res
    """
    try:
        if not UNIFIED_SYSTEM_AVAILABLE:
            raise HTTPException(status_code=503, detail="Syst√®me unifi√© non disponible")
        
        description = description_data.description
        
        description = description.strip()
        
        # R√©cup√©rer la t√¢che
        unified_manager = get_unified_task_manager()
        existing_task = unified_manager.get_task_by_id(task_id)
        
        if not existing_task:
            raise HTTPException(status_code=404, detail=f"T√¢che '{task_id}' non trouv√©e")
        
        # V√©rifier si changement n√©cessaire
        old_description = existing_task.get("description", "")
        if old_description == description:
            return {
                "message": "Description inchang√©e",
                "task_id": task_id,
                "description": description,
                "task": existing_task
            }
        
        # Pr√©parer mise √† jour
        updates = {
            "description": description,
            "updated_at": datetime.now().isoformat()
        }
        
        # Historique sp√©cifique (avec preview de changement)
        old_preview = old_description[:50] + "..." if len(old_description) > 50 else old_description
        new_preview = description[:50] + "..." if len(description) > 50 else description
        
        history_entry = {
            "action": "description_changed", 
            "timestamp": datetime.now().isoformat(),
            "user": "user",
            "details": f"Description modifi√©e: '{old_preview}' ‚Üí '{new_preview}'"
        }
        
        # Mettre √† jour
        updated_task = unified_manager.update_task(task_id, updates, history_entry)
        
        return {
            "message": "Description modifi√©e avec succ√®s",
            "task_id": task_id,
            "old_description": old_description,
            "new_description": description,
            "updated_at": updates["updated_at"],
            "task": updated_task
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur modification description: {str(e)}")

@app.patch("/tasks/{task_id}/department")
async def update_task_department(task_id: str, department: str = None):
    """
    üè¢ Modifier uniquement le d√©partement d'une t√¢che
    
    D√©partement : max 50 caract√®res ou null pour supprimer
    """
    try:
        if not UNIFIED_SYSTEM_AVAILABLE:
            raise HTTPException(status_code=503, detail="Syst√®me unifi√© non disponible")
        
        # Valider le d√©partement
        if department and len(department) > 50:
            raise HTTPException(status_code=400, detail="Nom de d√©partement trop long (maximum 50 caract√®res)")
        
        if department:
            department = department.strip()
            if not department:
                department = None
        
        # R√©cup√©rer la t√¢che
        unified_manager = get_unified_task_manager()
        existing_task = unified_manager.get_task_by_id(task_id)
        
        if not existing_task:
            raise HTTPException(status_code=404, detail=f"T√¢che '{task_id}' non trouv√©e")
        
        # V√©rifier si changement n√©cessaire
        old_department = existing_task.get("department")
        if old_department == department:
            return {
                "message": "D√©partement inchang√©",
                "task_id": task_id,
                "department": department,
                "task": existing_task
            }
        
        # Pr√©parer mise √† jour
        updates = {
            "department": department,
            "updated_at": datetime.now().isoformat()
        }
        
        # Historique sp√©cifique
        old_str = old_department or "non d√©fini"
        new_str = department or "supprim√©"
        history_entry = {
            "action": "department_changed",
            "timestamp": datetime.now().isoformat(),
            "user": "user",
            "details": f"D√©partement modifi√©: {old_str} ‚Üí {new_str}"
        }
        
        # Mettre √† jour
        updated_task = unified_manager.update_task(task_id, updates, history_entry)
        
        return {
            "message": "D√©partement modifi√© avec succ√®s",
            "task_id": task_id,
            "old_department": old_department,
            "new_department": department,
            "updated_at": updates["updated_at"],
            "task": updated_task
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur modification d√©partement: {str(e)}")

@app.post("/tasks/{task_id}/comment")
async def add_task_comment(task_id: str, comment_data: TaskComment):
    """
    üí¨ Ajouter un commentaire √† une t√¢che

    Le commentaire est ajout√© dans l'historique et dans une section comments d√©di√©e.
    Limite : 1-1000 caract√®res
    """
    try:
        if not UNIFIED_SYSTEM_AVAILABLE:
            raise HTTPException(status_code=503, detail="Syst√®me unifi√© non disponible")

        # R√©cup√©rer la t√¢che
        unified_manager = get_unified_task_manager()
        existing_task = unified_manager.get_task_by_id(task_id)

        if not existing_task:
            raise HTTPException(status_code=404, detail=f"T√¢che '{task_id}' non trouv√©e")

        # Pr√©parer le commentaire
        comment_timestamp = datetime.now().isoformat()
        comment = {
            "id": f"comment_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{str(uuid.uuid4())[:8]}",
            "content": comment_data.comment.strip(),
            "author": comment_data.author,
            "timestamp": comment_timestamp
        }

        # Ajouter le commentaire √† la liste des commentaires
        if "comments" not in existing_task:
            existing_task["comments"] = []

        updates = {
            "comments": existing_task["comments"] + [comment],
            "updated_at": comment_timestamp
        }

        # Historique sp√©cifique pour commentaire
        comment_preview = comment_data.comment[:100] + "..." if len(comment_data.comment) > 100 else comment_data.comment
        history_entry = {
            "action": "comment_added",
            "timestamp": comment_timestamp,
            "user": comment_data.author,
            "details": f"Commentaire ajout√©: '{comment_preview}'"
        }

        # Mettre √† jour la t√¢che
        updated_task = unified_manager.update_task(task_id, updates, history_entry)

        return {
            "message": "Commentaire ajout√© avec succ√®s",
            "task_id": task_id,
            "comment": comment,
            "total_comments": len(updated_task.get("comments", [])),
            "updated_at": comment_timestamp,
            "task": updated_task
        }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur ajout commentaire: {str(e)}")

# =====================================
# üìÑ NOUVEAUX ENDPOINTS - DOCUMENT PROCESSING
# =====================================

from fastapi import UploadFile, File
from typing import List
import os
import tempfile

@app.post("/documents/upload")
async def upload_document(
    file: UploadFile = File(...),
    extract_tasks: bool = True,
    use_cache: bool = True
):
    """
    üìÑ Upload et traitement de documents (PDF, Word, TXT)

    Fonctionnalit√©s :
    - Support PDF, Word (.docx), et fichiers texte
    - Extraction automatique du texte
    - Validation du type et taille de fichier
    - Extraction de t√¢ches optionnelle
    - Cache anti-doublon bas√© sur hash du contenu

    Param√®tres :
    - file: Fichier √† uploader
    - extract_tasks: Extraire les t√¢ches du document (d√©faut: True)
    - use_cache: Utiliser le cache (d√©faut: True)

    Retourne :
    - Texte extrait
    - T√¢ches extraites (si demand√©)
    - M√©tadonn√©es du fichier
    """
    try:
        # Validation du type de fichier
        allowed_extensions = ['.pdf', '.docx', '.txt', '.doc']
        file_extension = os.path.splitext(file.filename)[1].lower()

        if file_extension not in allowed_extensions:
            raise HTTPException(
                status_code=400,
                detail=f"Type de fichier non support√©. Extensions autoris√©es: {', '.join(allowed_extensions)}"
            )

        # Validation de la taille (max 10MB)
        max_size = 10 * 1024 * 1024  # 10MB
        file_content = await file.read()
        file_size = len(file_content)

        if file_size > max_size:
            raise HTTPException(
                status_code=400,
                detail=f"Fichier trop volumineux. Taille maximale: 10MB, taille actuelle: {file_size / (1024*1024):.1f}MB"
            )

        # Calcul du hash pour le cache
        import hashlib
        file_hash = hashlib.md5(file_content).hexdigest()[:16]

        # V√©rification cache si demand√©
        if use_cache and est_email_deja_traite(file_hash):
            info_cache = obtenir_info_cache(file_hash)
            return {
                "status": "cache_hit",
                "message": "Document similaire d√©j√† trait√©",
                "file_hash": file_hash,
                "file_info": {
                    "filename": file.filename,
                    "size": file_size,
                    "type": file_extension
                },
                "cached_result": info_cache
            }

        # Sauvegarde temporaire du fichier
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_extension) as temp_file:
            temp_file.write(file_content)
            temp_file_path = temp_file.name

        try:
            # Extraction du texte selon le type
            extracted_text = ""

            if file_extension == '.pdf':
                try:
                    import PyPDF2
                    with open(temp_file_path, 'rb') as pdf_file:
                        pdf_reader = PyPDF2.PdfReader(pdf_file)
                        for page in pdf_reader.pages:
                            extracted_text += page.extract_text() + "\n"
                except ImportError:
                    raise HTTPException(
                        status_code=500,
                        detail="Support PDF non disponible. Installez PyPDF2: pip install PyPDF2"
                    )

            elif file_extension in ['.docx', '.doc']:
                try:
                    from docx import Document
                    doc = Document(temp_file_path)
                    for paragraph in doc.paragraphs:
                        extracted_text += paragraph.text + "\n"
                except ImportError:
                    raise HTTPException(
                        status_code=500,
                        detail="Support Word non disponible. Installez python-docx: pip install python-docx"
                    )

            elif file_extension == '.txt':
                with open(temp_file_path, 'r', encoding='utf-8', errors='ignore') as txt_file:
                    extracted_text = txt_file.read()

            # Nettoyage du texte
            extracted_text = extracted_text.strip()
            if not extracted_text:
                raise HTTPException(
                    status_code=400,
                    detail="Aucun texte extractible trouv√© dans le document"
                )

            # Extraction de t√¢ches si demand√©
            tasks_extracted = []
            if extract_tasks:
                try:
                    # Utiliser la logique d'extraction existante
                    tasks_data = extract_tasks_from_email(extracted_text)
                    tasks_extracted = json.loads(tasks_data) if tasks_data else []

                    # Enrichissement des t√¢ches extraites du document
                    for task in tasks_extracted:
                        task["id"] = str(uuid.uuid4())
                        task["source"] = "document"
                        task["type"] = "document_processing"
                        task["document_info"] = {
                            "filename": file.filename,
                            "file_type": file_extension,
                            "file_size": file_size,
                            "file_hash": file_hash,
                            "uploaded_at": datetime.now().isoformat()
                        }
                        task["extrait_le"] = datetime.now().isoformat(timespec='seconds')
                        task["statut"] = "√† faire"

                        # Sauvegarde dans syst√®me unifi√© si disponible
                        if UNIFIED_SYSTEM_AVAILABLE:
                            unified_manager = get_unified_task_manager()
                            unified_task_data = {
                                "description": task["description"],
                                "responsable": task.get("responsable", "non sp√©cifi√©"),
                                "deadline": task.get("deadline"),
                                "priorite": task.get("priorite", "medium"),
                                "statut": task["statut"],
                                "source": "document",
                                "type": "document_processing",
                                "source_metadata": {
                                    "document_filename": file.filename,
                                    "document_type": file_extension,
                                    "document_size": file_size,
                                    "document_hash": file_hash,
                                    "extraction_timestamp": task["extrait_le"]
                                }
                            }
                            unified_manager.add_task(unified_task_data)

                except Exception as e:
                    print(f"‚ö†Ô∏è Erreur extraction t√¢ches: {e}")
                    # Ne pas √©chouer compl√®tement si l'extraction √©choue

            # Marquer dans le cache
            if use_cache:
                marquer_email_traite(file_hash, {
                    "filename": file.filename,
                    "file_type": file_extension,
                    "file_size": file_size,
                    "text_length": len(extracted_text),
                    "tasks_extracted": len(tasks_extracted),
                    "processed_at": datetime.now().isoformat()
                })

            return {
                "status": "success",
                "message": f"Document '{file.filename}' trait√© avec succ√®s",
                "file_info": {
                    "filename": file.filename,
                    "size": file_size,
                    "size_mb": round(file_size / (1024*1024), 2),
                    "type": file_extension,
                    "hash": file_hash
                },
                "extraction": {
                    "text_length": len(extracted_text),
                    "text_preview": extracted_text[:500] + "..." if len(extracted_text) > 500 else extracted_text,
                    "full_text": extracted_text
                },
                "tasks": {
                    "extracted": len(tasks_extracted),
                    "tasks": tasks_extracted
                },
                "processing_info": {
                    "cache_used": use_cache,
                    "cache_status": "new_document_added" if use_cache else "cache_disabled",
                    "processing_time": datetime.now().isoformat()
                }
            }

        finally:
            # Nettoyage du fichier temporaire
            if os.path.exists(temp_file_path):
                os.unlink(temp_file_path)

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur traitement document: {str(e)}")

@app.get("/documents/supported-formats")
async def get_supported_formats():
    """
    üìã Liste des formats de documents support√©s

    Retourne :
    - Formats accept√©s
    - Tailles maximales
    - Fonctionnalit√©s disponibles
    """
    return {
        "supported_formats": [
            {
                "extension": ".pdf",
                "description": "Documents PDF",
                "library": "PyPDF2",
                "max_size": "10MB"
            },
            {
                "extension": ".docx",
                "description": "Documents Word modernes",
                "library": "python-docx",
                "max_size": "10MB"
            },
            {
                "extension": ".doc",
                "description": "Documents Word classiques",
                "library": "python-docx",
                "max_size": "10MB"
            },
            {
                "extension": ".txt",
                "description": "Fichiers texte",
                "library": "built-in",
                "max_size": "10MB"
            }
        ],
        "features": [
            "Extraction automatique du texte",
            "Validation de type et taille",
            "Cache anti-doublon",
            "Extraction de t√¢ches IA",
            "Support syst√®me unifi√©"
        ],
        "limitations": [
            "Taille maximale: 10MB par fichier",
            "Encodage UTF-8 recommand√© pour les .txt",
            "Pas de support OCR pour les images dans PDF"
        ]
    }

if __name__ == "__main__":
    import uvicorn
    print("üåü D√©marrage serveur AI Task Extraction...")
    uvicorn.run(app, host="127.0.0.1", port=8000, reload=True)
